{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6169743",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "038e91c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acaa0560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7302e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import os, sys, shutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from glob import glob\n",
    "import random\n",
    "import cv2 as cv\n",
    "import torch.nn.functional as F\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82a9d0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['metadata.csv', 'data_df.csv', 'records']\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/media/mountHDD3/data_storage/biomedical_data/ecg_data/SPH\"\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f58c040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20838, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df = pd.read_csv(data_dir + \"/data_df.csv\")\n",
    "main_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1410107a",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_fns = main_df[\"File name\"].values.tolist()\n",
    "single_mat_paths = [data_dir + f\"/records/{x}.h5\" for x in single_fns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10056b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = [0.9, 0.1]\n",
    "\n",
    "train_index = int(len(single_mat_paths)*ratio[0])\n",
    "valid_index = int(len(single_mat_paths)*(ratio[0]+ratio[1]))\n",
    "\n",
    "train_mat_paths = single_mat_paths[:train_index]\n",
    "valid_mat_paths = single_mat_paths[train_index:valid_index]\n",
    "# test_mat_paths = single_mat_paths[valid_index:]\n",
    "\n",
    "train_label = main_df.iloc[:train_index,:]\n",
    "valid_label = main_df.iloc[train_index:valid_index,:]\n",
    "# test_label = single_main_df.iloc[valid_index:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6db91a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/mountHDD2/thao/git/HeartResearch/Chapman Dataset/2D_model/Proposed_model\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3985befb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/media/mountHDD2/thao/git/HeartResearch/Chapman Dataset/2D_model/Proposed_model/IWL_loss\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189b94c0",
   "metadata": {},
   "source": [
    " # Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39c47eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.nn.functional import one_hot\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c33309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeartData(Dataset):\n",
    "    def __init__(self, data_paths, label_df):\n",
    "        self.data_paths = data_paths\n",
    "        random.shuffle(self.data_paths)\n",
    "        self.label_df = label_df\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224), antialias=None),\n",
    "            normalize\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_path = self.data_paths[idx]\n",
    "        a = h5py.File(data_path, 'r')\n",
    "        data_h5 = a['ecg']\n",
    "        data = np.array(data_h5)\n",
    "        clip_data = data[:, 500:3000]\n",
    "        clip_data = torch.tensor(clip_data, dtype=torch.float32)\n",
    "        normalized_data = (clip_data - clip_data.min()) / (clip_data.max() - clip_data.min())\n",
    "        grayscale_images = (normalized_data * 255)\n",
    "        grayscale_images = grayscale_images.unsqueeze(0).unsqueeze(0) # (1, 1, h, w)\n",
    "        resized_images = F.interpolate(grayscale_images, size=(9*4,2500), mode='bilinear', align_corners=True)\n",
    "        resized_images = resized_images.squeeze(0).squeeze(0)\n",
    "        torch_data = resized_images.unsqueeze(0).repeat(3, 1, 1)\n",
    "        torch_data_resize = self.transform(torch_data)\n",
    "\n",
    "        filename = data_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        label = self.label_df[self.label_df[\"File name\"] == filename][\"New Label\"].values.item()\n",
    "\n",
    "        return torch_data_resize, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7adad047",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = HeartData(train_mat_paths, main_df)\n",
    "valid_ds = HeartData(valid_mat_paths, main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75847351",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\", index = 0)\n",
    "# device = (\"cpu\")\n",
    "batch_size = 64\n",
    "\n",
    "traindl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    pin_memory=True, \n",
    "    num_workers=os.cpu_count()//2\n",
    ")\n",
    "\n",
    "validdl = DataLoader(\n",
    "    valid_ds,\n",
    "    batch_size=1, \n",
    "    shuffle=True, \n",
    "    pin_memory=True, \n",
    "    num_workers=os.cpu_count()//2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad0e825",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "651dfeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from torch import nn\n",
    "\n",
    "class HeartModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ori_model = efficientnet_b0(weights = EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        self.ori_model.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1280, 34)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        x = self.ori_model(x)\n",
    "        return x\n",
    "\n",
    "model = HeartModel()\n",
    "# x = torch.randn((1, 3, 256, 512)).to(\"cuda\")\n",
    "# out = model(x)\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3843ece2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeartModel(\n",
       "  (ori_model): EfficientNet(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (8): Conv2dNormActivation(\n",
       "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Sequential(\n",
       "      (0): Dropout(p=0.2, inplace=False)\n",
       "      (1): Linear(in_features=1280, out_features=34, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe4995",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f66b6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 200\n",
    "lr = 0.0005 # lr = 0.001 Acc: 0.821875 lr = 0.0005 Acc: 0.825 Focal_loss Acc: 0.828\n",
    "best_acc = 0\n",
    "best_ep = 0\n",
    "\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "scheduler = CosineAnnealingLR(optimizer=optimizer, T_max=epoch*len(traindl))\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7811770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FocalLoss(nn.Module):\n",
    "#     def __init__(self, gamma=2, weight=None):\n",
    "#         super(FocalLoss, self).__init__()\n",
    "#         self.gamma = gamma\n",
    "#         self.weight = weight\n",
    "# #         self.reduction = reduction\n",
    "\n",
    "#     def forward(self, inputs, target):\n",
    "#         \"\"\"\n",
    "#         input: [N, C], float32\n",
    "#         target: [N, ], int64\n",
    "#         \"\"\"\n",
    "#         logpt = F.log_softmax(inputs, dim=1)\n",
    "#         pt = torch.exp(logpt)\n",
    "#         logpt = (1-pt)**self.gamma * logpt\n",
    "#         loss = F.nll_loss(logpt, target, self.weight)\n",
    "#         return loss\n",
    "    \n",
    "# focalloss_fn = FocalLoss()\n",
    "\n",
    "# 0.1 - 0.001 - 82.1\n",
    "# 0.1 - 0.0005 - 81.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c23f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalClassifierV0(nn.Module):\n",
    "    def __init__(self, gamma=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.act = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "\n",
    "        logits = self.act(pred)\n",
    "\n",
    "        B, C = tuple(logits.size())\n",
    "\n",
    "        entropy = torch.pow(1 - logits, self.gamma) * logits * F.one_hot(target, num_classes=C).float()\n",
    "\n",
    "        return (-1 / B) * torch.sum(entropy)\n",
    "\n",
    "focalloss_fn = FocalClassifierV0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6223c1-eb88-4b9b-adeb-a571486577bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "294it [00:33,  8.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.1222862174079686 - train acc: 74.60275141303188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2084it [00:17, 117.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.7866700986635033 - valid acc: 77.447216890595\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "294it [00:24, 11.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.7299546020633124 - train acc: 80.48416337847925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2084it [00:12, 165.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.8217354144244203 - valid acc: 75.81573896353166\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "294it [00:24, 11.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.6060813054594977 - train acc: 83.01695638263837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2084it [00:12, 162.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.6935134384749839 - valid acc: 81.14203454894434\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "294it [00:25, 11.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.5013156444749735 - train acc: 85.09118054814972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2084it [00:13, 160.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.6530377181488335 - valid acc: 81.19001919385796\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "294it [00:25, 11.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.42075148488016667 - train acc: 87.22939106323985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2084it [00:12, 162.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.6954426292476303 - valid acc: 81.14203454894434\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "294it [00:25, 11.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.3293212605926364 - train acc: 89.83150261277594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2084it [00:13, 160.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.8010129260841578 - valid acc: 80.56621880998081\n",
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "294it [00:25, 11.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.2955369522140295 - train acc: 90.6846539404927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2084it [00:12, 161.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.7879554745446588 - valid acc: 81.14203454894434\n",
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "294it [00:24, 11.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.2281217162494163 - train acc: 92.67356297323238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2084it [00:12, 163.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.8991937782984283 - valid acc: 80.71017274472169\n",
      "Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "294it [00:25, 11.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1725923593184647 - train acc: 95.1743628026021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2084it [00:13, 159.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.9195823669000079 - valid acc: 78.69481765834932\n",
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "294it [00:25, 11.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.18349476162433218 - train acc: 94.33187586648182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2084it [00:13, 160.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.9181074282415338 - valid acc: 79.41458733205374\n",
      "Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "294it [00:25, 11.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.12502783872886425 - train acc: 96.63005225551883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2084it [00:13, 159.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 1.053461159906095 - valid acc: 81.33397312859884\n",
      "Epoch: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "33it [00:03, 12.02it/s]"
     ]
    }
   ],
   "source": [
    "for e in range(epoch):\n",
    "    model.train()\n",
    "    print(f\"Epoch: {e}\")\n",
    "    batch_cnt = 0\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for batch, (train_sig, train_label) in tqdm(enumerate(traindl)):\n",
    "        batch_cnt = batch\n",
    "        train_sig = train_sig.to(device)\n",
    "        train_label = train_label.to(device)\n",
    "        \n",
    "        pred = model(train_sig)\n",
    "        loss = focalloss_fn(pred, train_label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == train_label).type(torch.float).sum().item()\n",
    "    \n",
    "    total_loss /= batch_cnt\n",
    "    correct /= len(traindl.dataset)\n",
    "    \n",
    "    print(f\"train loss: {total_loss} - train acc: {100*correct}\")\n",
    "    \n",
    "    batch_cnt = 0\n",
    "    val_total_loss = 0\n",
    "    val_correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch, (valid_sig, valid_label) in tqdm(enumerate(validdl)):\n",
    "            batch_cnt = batch\n",
    "            valid_sig = valid_sig.to(device)\n",
    "            valid_label = valid_label.to(device)\n",
    "            \n",
    "            pred = model(valid_sig)\n",
    "            loss = loss_fn(pred, valid_label)\n",
    "            \n",
    "            val_total_loss += loss.item()\n",
    "            val_correct += (pred.argmax(1) == valid_label).type(torch.float).sum().item()\n",
    "    \n",
    "        val_total_loss /= batch_cnt\n",
    "        val_correct /= len(validdl.dataset)\n",
    "        if val_correct > best_acc:\n",
    "            best_acc = val_correct\n",
    "            best_ep = e\n",
    "        \n",
    "        print(f\"valid loss: {val_total_loss} - valid acc: {100*val_correct}\")\n",
    "        \n",
    "print(f\"Best acuracy: {best_acc} at epoch {best_ep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4725f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_plot(train_losses, val_losses, n_epochs, check_folder):\n",
    "    now = datetime.now().strftime(\"%m-%d-%Y - %H-%M-%S\")\n",
    "    run_dir = save_dir + f\"/{check_folder}\"\n",
    "    if not os.path.exists(run_dir):\n",
    "        os.mkdir(run_dir)\n",
    "    save_loss_dir = run_dir + \"/save_losses\"\n",
    "    if not os.path.exists(save_loss_dir):\n",
    "        os.mkdir(save_loss_dir)\n",
    "    save_fig_losses = os.path.join(save_loss_dir, f\"plot_losses_epoch{n_epochs}_{now}.png\")  \n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(epoch), np.array(train_losses), label='Train Loss')\n",
    "    plt.plot(range(epoch), np.array(val_losses), label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    # plt.xticks()\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Test Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(save_fig_losses)\n",
    "    \n",
    "\n",
    "def acc_plot(train_cls_acc, val_cls_acc, n_epochs, check_folder):\n",
    "    now = datetime.now().strftime(\"%m-%d-%Y - %H-%M-%S\")\n",
    "    run_dir = os.getcwd() + f\"/{check_folder}\"\n",
    "    if not os.path.exists(run_dir):\n",
    "        os.mkdir(run_dir)\n",
    "#         save_dir = run_dir + f\"/{now}\"\n",
    "#         if not os.path.exists(save_dir):\n",
    "#             os.mkdir(save_dir)\n",
    "    save_acc_dir = run_dir + \"/save_acc\"\n",
    "    if not os.path.exists(save_acc_dir):\n",
    "        os.mkdir(save_acc_dir)\n",
    "    save_fig_acc = os.path.join(save_acc_dir, 'plot_acc_epoch{}_{}.png'.format(n_epochs, now))  \n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(epoch), np.array(train_cls_acc), label='Train Accuracy')\n",
    "    plt.plot(range(epoch), np.array(val_cls_acc), label='Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    # plt.xticks\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Test Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(save_fig_acc)\n",
    "\n",
    "    \n",
    "def checkpoint(valid_class_acc, \n",
    "               val_total_loss,\n",
    "               old_valid_class_acc,\n",
    "               old_valid_loss,\n",
    "               epoch, \n",
    "               model,\n",
    "               optimizer,\n",
    "               check_folder\n",
    "#                    logs\n",
    "              ):\n",
    "\n",
    "    if valid_class_acc >= old_valid_class_acc and val_total_loss <= old_valid_loss:\n",
    "        old_valid_class_acc = valid_class_acc\n",
    "        old_valid_loss = val_total_loss\n",
    "        save_dict = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': val_total_loss,\n",
    "            'test_acc': valid_class_acc\n",
    "        }\n",
    "\n",
    "     # Saving best model\n",
    "        now = datetime.now().strftime(\"%m-%d-%Y - %H-%M-%S\")\n",
    "        run_dir = os.getcwd() + f\"/{check_folder}\"\n",
    "        if not os.path.exists(run_dir):\n",
    "            os.mkdir(run_dir)\n",
    "    #         save_dir = run_dir + f\"/{now}\"\n",
    "    #         if not os.path.exists(save_dir):\n",
    "    #             os.mkdir(save_dir)\n",
    "        save_best_model_dir = run_dir + \"/save_best_model\"\n",
    "        if not os.path.exists(save_best_model_dir):\n",
    "            os.mkdir(save_best_model_dir)\n",
    "        save_best_model_path = save_best_model_dir + f\"/{save_dict['loss']:>7f}_{save_dict['test_acc']:>7f}_{now}.pt\"\n",
    "        torch.save(save_dict, save_best_model_path)\n",
    "        \n",
    "def classification_report_csv(report, auc, check_folder):\n",
    "    now = datetime.now().strftime(\"%m-%d-%Y - %H-%M-%S\")\n",
    "    run_dir = save_dir + f\"/{check_folder}\"\n",
    "    if not os.path.exists(run_dir):\n",
    "        os.mkdir(run_dir)\n",
    "    save_report_dir = run_dir + \"/save_classification_report\"\n",
    "    if not os.path.exists(save_report_dir):\n",
    "        os.mkdir(save_report_dir)\n",
    "        \n",
    "    report_data = report['macro avg']\n",
    "    del report_data['support']\n",
    "    report_data.update({'auc': auc})\n",
    "#     print(type(report_data))\n",
    "#     print(report_data)\n",
    "\n",
    "#     dataframe = pd.DataFrame.from_dict(report_data, orient='index')\n",
    "#     save_report_file = save_report_dir + f\"/classification_report_{now}.csv\"\n",
    "#     dataframe.to_csv(save_report_file, index = False)\n",
    "    with open(save_report_dir + f\"/cls_report_{now}.json\", \"w\") as outfile: \n",
    "        json.dump(report_data, outfile)\n",
    "    \n",
    "def acc_loss_json(log_dict, check_folder):\n",
    "    now = datetime.now().strftime(\"%m-%d-%Y - %H-%M-%S\")\n",
    "    run_dir = save_dir + f\"/{check_folder}\"\n",
    "    if not os.path.exists(run_dir):\n",
    "        os.mkdir(run_dir)\n",
    "    save_json_dir = run_dir + \"/save_acc_loss_json\"\n",
    "    if not os.path.exists(save_json_dir):\n",
    "        os.mkdir(save_json_dir) \n",
    "    save_acc_loss_file = save_json_dir + f\"/acc_loss_{now}.json\"\n",
    "    with open(save_acc_loss_file, \"w\") as outfile: \n",
    "        json.dump(log_dict, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ca150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict = {\n",
    "    \"train\": {\n",
    "        \"acc\": [],\n",
    "        \"loss\": []\n",
    "    },\n",
    "    \"valid\": {\n",
    "        \"acc\": [],\n",
    "        \"loss\": []\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8989c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "class_la = [1,2,3,4,5,6,7,8,9]\n",
    "train_losses = []\n",
    "train_acc_plot = []\n",
    "val_losses = []\n",
    "val_acc_plot = []\n",
    "old_valid_class_acc = 0\n",
    "old_valid_loss = 1e23\n",
    "for e in range(epoch):\n",
    "    model.train()\n",
    "    print(f\"Epoch: {e+1}\")\n",
    "    batch_cnt = 0\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for batch, (train_sig, train_label) in tqdm(enumerate(traindl)):\n",
    "        batch_cnt = batch\n",
    "        train_sig = train_sig.to(device)\n",
    "        train_label = train_label.to(device)\n",
    "        \n",
    "        pred = model(train_sig)\n",
    "        loss = focalloss_fn(pred, train_label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == train_label).type(torch.float).sum().item()\n",
    "        \n",
    "    total_loss /= batch_cnt\n",
    "    correct /= len(traindl.dataset)\n",
    "    log_dict[\"train\"][\"loss\"].append(total_loss)\n",
    "    log_dict[\"train\"][\"acc\"].append(correct)\n",
    "    \n",
    "    print(f\"train loss: {total_loss} - train acc: {100*correct}\")\n",
    "\n",
    "# Valid\n",
    "    batch_cnt = 0\n",
    "    val_total_loss = 0\n",
    "    val_correct = 0\n",
    "    model.eval()\n",
    "    y_true_list = [] \n",
    "    pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch, (valid_sig, valid_label) in tqdm(enumerate(validdl)):\n",
    "            batch_cnt = batch\n",
    "            valid_label = valid_label.to(device)\n",
    "            \n",
    "            valid_sig = valid_sig.to(device)  \n",
    "            pred = model(valid_sig)\n",
    "            pred_pos = pred.argmax(1)\n",
    "            \n",
    "            y_true_list.append(valid_label)\n",
    "            pred_list.append(pred_pos)\n",
    "            \n",
    "            loss = loss_fn(pred, valid_label)\n",
    "            val_total_loss += loss.item()\n",
    "            val_correct += (pred.argmax(1) == valid_label).type(torch.float).sum().item()\n",
    "            \n",
    "\n",
    "            \n",
    "        val_total_loss /= batch_cnt\n",
    "        val_correct /= len(validdl.dataset)\n",
    "        log_dict[\"valid\"][\"loss\"].append(val_total_loss)\n",
    "        log_dict[\"valid\"][\"acc\"].append(val_correct)\n",
    "\n",
    "        if val_correct > best_acc:\n",
    "            best_acc = val_correct\n",
    "            best_ep = e \n",
    "\n",
    "        print(f\"valid loss: {val_total_loss} - valid acc: {100*val_correct}\")\n",
    "#         checkpoint(valid_class_acc = val_correct, \n",
    "#                    val_total_loss = val_total_loss,\n",
    "#                    old_valid_class_acc = old_valid_class_acc ,\n",
    "#                    old_valid_loss = old_valid_loss,\n",
    "#                    epoch = e, \n",
    "#                    model = model,\n",
    "#                    optimizer = optimizer,\n",
    "#                    check_folder = checkpoint_folder)\n",
    "        if val_correct >= old_valid_class_acc and val_total_loss <= old_valid_loss:\n",
    "            old_valid_class_acc = val_correct\n",
    "            old_valid_loss = val_total_loss\n",
    "            save_dict = {\n",
    "                'epoch': e,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': val_total_loss,\n",
    "                'test_acc': val_correct\n",
    "            }\n",
    "\n",
    "         # Saving best model\n",
    "            now = datetime.now().strftime(\"%m-%d-%Y - %H-%M-%S\")\n",
    "            run_dir = os.getcwd() + f\"/{checkpoint_folder}\"\n",
    "            if not os.path.exists(run_dir):\n",
    "                os.mkdir(run_dir)\n",
    "            save_best_model_dir = run_dir + \"/save_best_model\"\n",
    "            if not os.path.exists(save_best_model_dir):\n",
    "                os.mkdir(save_best_model_dir)\n",
    "            save_best_model_path = save_best_model_dir + f\"/{save_dict['loss']:>7f}_{save_dict['test_acc']:>7f}_{now}.pt\"\n",
    "            torch.save(save_dict, save_best_model_path)\n",
    "        \n",
    "print(f\"Best acuracy: {best_acc} at epoch {best_ep}\")\n",
    "\n",
    "y_true = torch.cat(y_true_list).cpu().numpy()\n",
    "pred = torch.cat(pred_list).cpu().numpy()\n",
    "# print(type(y_true))\n",
    "print(y_true)\n",
    "print(f\"pred: {pred}\")\n",
    "\n",
    "# Save loss and acc to json file\n",
    "acc_loss_json(log_dict, check_folder = checkpoint_folder)\n",
    "    \n",
    "# Save plot acc and loss\n",
    "# loss_plot(train_losses = train_losses, \n",
    "#           val_losses = val_losses, n_epochs = e , \n",
    "#           check_folder = checkpoint_folder)\n",
    "# acc_plot(train_cls_acc = train_acc_plot , val_cls_acc = val_acc_plot, \n",
    "#                   n_epochs = e, check_folder = checkpoint_folder)\n",
    "\n",
    "# AUC\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_true, pred, pos_label = 0)\n",
    "# print(fpr)\n",
    "# print(f\"tpr: {tpr}\")\n",
    "auc1 = metrics.auc(fpr, tpr)\n",
    "\n",
    "# print(f\"hihi {auc1}\")\n",
    "# print(type(auc1))\n",
    "\n",
    "# Classification report\n",
    "for i in range (len(class_la)):\n",
    "    class_la[i] = str(class_la[i])\n",
    "\n",
    "reports = classification_report(y_true, pred, target_names=class_la, output_dict=True) \n",
    "# print(reports)\n",
    "classification_report_csv(report = reports, auc = auc1, check_folder = checkpoint_folder)\n",
    "\n",
    "# json_object = json.dumps(reports, indent = 4) \n",
    "# print(json_object)\n",
    "\n",
    "\n",
    "\n",
    "# ROC Curve\n",
    "# fpr = [0] * 9\n",
    "# tpr = [0] * 9\n",
    "# thresholds = [0] * 9\n",
    "# auc_score = [0] * 9\n",
    " \n",
    "# for i in range(9):\n",
    "#     fpr[i], tpr[i], thresholds[i] = roc_curve(y_true[:, i],\n",
    "#                                               pred[:, i])\n",
    "#     auc_score[i] = auc(fpr[i], tpr[i])\n",
    "# print(auc_score)\n",
    "# for i in range(9):\n",
    "#     r2 = roc_auc_score(y_true, pred, average='micro', multi_class='ovr')\n",
    "#     print(\"The ROC AUC score of \"+ class_la[i] +\" is: \"+str(r2))\n",
    "\n",
    "                    # Compute ROC curve and ROC area for each class\n",
    "# class_fpr = {}\n",
    "# class_tpr = {}\n",
    "# class_roc_auc = dict()\n",
    "# for i in range(9):\n",
    "#     class_fpr[i], class_tpr[i], _ = roc_curve(valid_label[:, i].cpu(), pred[:, i].cpu(), drop_intermediate=False)\n",
    "#     roc_auc[i] = auc(class_fpr[i], class_tpr[i])\n",
    "\n",
    "# now = datetime.now().strftime(\"%m-%d-%Y - %H-%M-%S\")\n",
    "# run_dir = save_dir + f\"/{check_folder}\"\n",
    "# if not os.path.exists(run_dir):\n",
    "#     os.mkdir(run_dir)\n",
    "# save_ROC_dir = run_dir + f\"/save_ROC_{now}\"\n",
    "# if not os.path.exists(save_ROC_dir):\n",
    "#     os.mkdir(save_ROC_dir)\n",
    "\n",
    "# plt.plot(class_fpr[0], class_tpr[0],'turquoise',label='1: ROC curve of class 1 (area = %0.2f)' % roc_auc[0])\n",
    "# plt.plot(class_fpr[1], class_tpr[1],'peachpuff',label='2: ROC curve of class 2 (area = %0.2f)' % roc_auc[1])\n",
    "# plt.plot(class_fpr[2], class_tpr[2],'paleturquoise',label='3: ROC curve of class 3 (area = %0.2f)' % roc_auc[2])\n",
    "# plt.plot(class_fpr[3], class_tpr[3],'pink',label='4: ROC curve of class 4 (area = %0.2f)' % roc_auc[3])\n",
    "# plt.plot(class_fpr[4], class_tpr[4],'lightcoral',label='5: ROC curve of class 5 (area = %0.2f)' % roc_auc[4])\n",
    "# plt.plot(class_fpr[5], class_tpr[5],'peachpuff',label='6: ROC curve of class 6 (area = %0.2f)' % roc_auc[5])\n",
    "# plt.plot(class_fpr[6], class_tpr[6],'steelblue',label='7: ROC curve of class 7 (area = %0.2f)' % roc_auc[6])\n",
    "# plt.plot(class_fpr[7], class_tpr[7],'forestgreen',label='8: ROC curve of class 8 (area = %0.2f)' % roc_auc[6])\n",
    "# plt.plot(class_fpr[8], class_tpr[8],'darkslategray',label='9: ROC curve of class 9 (area = %0.2f)' % roc_auc[6])\n",
    "\n",
    "# plt.plot([0, 1], [0, 1], 'k--')\n",
    "# plt.xlim([-0.1, 1.1])\n",
    "# plt.ylim([-0.1, 1.1])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver operating characteristic of lead')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.savefig(save_ROC_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04d253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(reports.keys())\n",
    "# print(reports['macro avg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df57c6c",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
