{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6169743",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "038e91c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acaa0560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7302e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import os, sys, shutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from glob import glob\n",
    "import random\n",
    "import cv2 as cv\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80737247-a198-4481-bb11-c775cce8918f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/mountHDD2/thao/git/HeartResearch/Chapman Dataset/2D_model/Proposed_model\n",
      "['Data_Explore', 'Diagnostics.xlsx', '.ipynb_checkpoints', '2D_model', '1D_Model', '2D_Loss']\n",
      "/media/mountHDD2/thao/git/HeartResearch/Chapman Dataset/2D_model/Proposed_model\n"
     ]
    }
   ],
   "source": [
    "# main_data_dir =  \"/media/mountHDD2\"\n",
    "print(os.getcwd())\n",
    "save_dir = os.getcwd()\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "main_dir = os.getcwd() \n",
    "print(os.listdir(main_dir))\n",
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "822504e0-fad3-48b2-95d3-0d5f51c2a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/media/mountHDD3/data_storage/biomedical_data/ecg_data/ECGDataDenoised\"\n",
    "label_file = main_dir + \"/Diagnostics.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec1aa643-c6d4-47e2-8e5f-9740dcbf985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df = pd.read_csv(data_dir + \"/MUSE_20180120_121711_19000.csv\")\n",
    "label_df = pd.read_excel(label_file)\n",
    "label_df = label_df[['FileName', 'Rhythm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad2806b3-c6ba-47ba-9713-6ed3a2657d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         FileName  Rhythm\n",
      "0      MUSE_20180120_121711_19000       5\n",
      "1      MUSE_20180120_121704_86000       5\n",
      "2      MUSE_20180113_125357_13000       5\n",
      "3      MUSE_20180113_134825_04000       7\n",
      "4      MUSE_20180115_123455_79000       7\n",
      "...                           ...     ...\n",
      "10641  MUSE_20181222_204246_47000      10\n",
      "10642  MUSE_20180115_120332_79000       5\n",
      "10643  MUSE_20180712_152507_30000       0\n",
      "10644  MUSE_20180118_181350_17000       5\n",
      "10645  MUSE_20180116_121646_28000       9\n",
      "\n",
      "[10646 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_df['Rhythm'] = label_encoder.fit_transform(label_df['Rhythm'])\n",
    "print(label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "551f03a0-8686-42da-8ab5-76c1c41b477c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10646\n"
     ]
    }
   ],
   "source": [
    "filenames = label_df[\"FileName\"].values.tolist()\n",
    "print(len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22419d37-c440-4859-9763-f9fdd9906d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "unique_values = np.unique(label_df[\"Rhythm\"].values.tolist())\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb86b7df-ea31-4428-9b40-d24fe4e3b018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/mountHDD3/data_storage/biomedical_data/ecg_data/ECGDataDenoised/MUSE_20180120_121711_19000.csv\n",
      "MUSE_20180120_121711_19000\n",
      "10646\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "data_paths = [data_dir + f\"/{x}.csv\" for x in filenames]\n",
    "print(data_paths[0])\n",
    "print(data_paths[0].split(\"/\")[-1].split(\".\")[0])\n",
    "print(len(data_paths))\n",
    "data_test = pd.read_csv(data_paths[0], header = None)\n",
    "print(type(data_test.values.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82a9d0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "# data_dir = \"/media/mountHDD2/khoibaocon\"\n",
    "# print(os.listdir(data_dir))\n",
    "print(type(label_df[\"Rhythm\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f58c040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_df = pd.read_csv(data_dir + \"/Label.csv\")\n",
    "# main_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1410107a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_main_df = main_df[main_df[\"Second_label\"].isnull()]\n",
    "# single_fns = single_main_df[\"Recording\"].values.tolist()\n",
    "# single_mat_paths = [data_dir + f\"/alldata/{x}.mat\" for x in single_fns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10056b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8516\n",
      "8516\n"
     ]
    }
   ],
   "source": [
    "ratio = [0.8, 0.1]\n",
    "\n",
    "train_index = int(len(data_paths)*ratio[0])\n",
    "valid_index = int(len(data_paths)*(ratio[0]+ratio[1]))\n",
    "print(train_index)\n",
    "train_mat_paths = data_paths[:train_index]\n",
    "valid_mat_paths = data_paths[train_index:valid_index]\n",
    "print(len(train_mat_paths))\n",
    "\n",
    "train_label = label_df.iloc[:train_index,:]\n",
    "valid_label = label_df.iloc[train_index:valid_index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "864b1ccf-2b0a-4197-b739-39a1cb009ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8516\n",
      "1065\n"
     ]
    }
   ],
   "source": [
    "print(len(train_label))\n",
    "print(len(valid_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f84c9a40-20c6-4943-a86b-9f42ef0ada45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, ..., 0, 5, 9])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df[label_df[\"FileName\"] == filenames][\"Rhythm\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189b94c0",
   "metadata": {},
   "source": [
    " # Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39c47eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.nn.functional import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c33309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeartData(Dataset):\n",
    "    def __init__(self, data_paths, label_df):\n",
    "        self.data_paths = data_paths\n",
    "        random.shuffle(self.data_paths)\n",
    "        self.label_df = label_df\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224), antialias=None),\n",
    "            normalize\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_path = self.data_paths[idx]        \n",
    "        # data = loadmat(data_path)['ECG'][0][0][2]\n",
    "        data = pd.read_csv(data_path, header = None)\n",
    "        data = data.values.T\n",
    "        clip_data = data[:, 500:3000]\n",
    "        clip_data = torch.tensor(clip_data, dtype=torch.float32)\n",
    "        normalized_data = (clip_data - clip_data.min()) / (clip_data.max() - clip_data.min())\n",
    "        grayscale_images = (normalized_data * 255)\n",
    "        grayscale_images = grayscale_images.unsqueeze(0).unsqueeze(0) # (1, 1, h, w)\n",
    "        resized_images = F.interpolate(grayscale_images, size=(9*4,2500), mode='bilinear', align_corners=True)\n",
    "        resized_images = resized_images.squeeze(0).squeeze(0)\n",
    "        torch_data = resized_images.unsqueeze(0).repeat(3, 1, 1)\n",
    "        torch_data_resize = self.transform(torch_data)\n",
    "\n",
    "        filename = data_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        label = self.label_df[self.label_df[\"FileName\"] == filename][\"Rhythm\"].values.item()\n",
    "\n",
    "        # return torch_data_resize, label-1\n",
    "        return torch_data_resize, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7adad047",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = HeartData(train_mat_paths, label_df)\n",
    "valid_ds = HeartData(valid_mat_paths, label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75847351",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\", index = 1)\n",
    "batch_size = 64\n",
    "\n",
    "traindl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    pin_memory=True, \n",
    "    num_workers=os.cpu_count()//2\n",
    ")\n",
    "\n",
    "validdl = DataLoader(\n",
    "    valid_ds,\n",
    "    batch_size=1, \n",
    "    # shuffle=True, \n",
    "    pin_memory=True, \n",
    "    num_workers=os.cpu_count()//2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad0e825",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "651dfeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "tensor(2.6415, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from torch import nn\n",
    "\n",
    "class HeartModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ori_model = efficientnet_b0(weights = EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        self.ori_model.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1280, 11),\n",
    "            # nn.Softmax(dim = 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        x = self.ori_model(x)\n",
    "        return x\n",
    "model = HeartModel()\n",
    "x = torch.randn((1, 3, 224, 224))\n",
    "y = torch.randint(0, 10, (1,))\n",
    "print(y.shape)\n",
    "out = model(x)\n",
    "print(nn.CrossEntropyLoss()(out, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3843ece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe4995",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f66b6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 150\n",
    "lr = 0.001 # lr = 0.001 Acc: 0.821875 lr = 0.0005 Acc: 0.825 Focal_loss Acc: 0.828\n",
    "best_acc = 0\n",
    "best_ep = 0\n",
    "\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "scheduler = CosineAnnealingLR(optimizer=optimizer, T_max=epoch*len(traindl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c23f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalClassifierV0(nn.Module):\n",
    "    def __init__(self, gamma=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.act = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "\n",
    "        logits = self.act(pred)\n",
    "\n",
    "        B, C = tuple(logits.size())\n",
    "\n",
    "        entropy = torch.pow(1 - logits, self.gamma) * logits * F.one_hot(target, num_classes=C).float()\n",
    "\n",
    "        return (-1 / B) * torch.sum(entropy)\n",
    "\n",
    "focalloss_fn = FocalClassifierV0()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# checkpoint_folder = \"run_efficientB0_heatmap_gamma0.5_lr0.0005\"\n",
    "# checkpoint_folder = \"run_proposed_gamma0.3_0.01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4725f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_plot(train_losses, val_losses, n_epochs, check_folder):\n",
    "    now = datetime.now().strftime(\"%m-%d-%Y - %H-%M-%S\")\n",
    "    run_dir = save_dir + f\"/{check_folder}\"\n",
    "    if not os.path.exists(run_dir):\n",
    "        os.mkdir(run_dir)\n",
    "    save_loss_dir = run_dir + \"/save_losses\"\n",
    "    if not os.path.exists(save_loss_dir):\n",
    "        os.mkdir(save_loss_dir)\n",
    "    save_fig_losses = os.path.join(save_loss_dir, f\"plot_losses_epoch{n_epochs}_{now}.png\")  \n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(epoch), np.array(train_losses), label='Train Loss')\n",
    "    plt.plot(range(epoch), np.array(val_losses), label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    # plt.xticks()\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Test Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(save_fig_losses)\n",
    "    \n",
    "\n",
    "def acc_plot(train_cls_acc, val_cls_acc, n_epochs, check_folder):\n",
    "    now = datetime.now().strftime(\"%m-%d-%Y - %H-%M-%S\")\n",
    "    run_dir = os.getcwd() + f\"/{check_folder}\"\n",
    "    if not os.path.exists(run_dir):\n",
    "        os.mkdir(run_dir)\n",
    "#         save_dir = run_dir + f\"/{now}\"\n",
    "#         if not os.path.exists(save_dir):\n",
    "#             os.mkdir(save_dir)\n",
    "    save_acc_dir = run_dir + \"/save_acc\"\n",
    "    if not os.path.exists(save_acc_dir):\n",
    "        os.mkdir(save_acc_dir)\n",
    "    save_fig_acc = os.path.join(save_acc_dir, 'plot_acc_epoch{}_{}.png'.format(n_epochs, now))  \n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(epoch), np.array(train_cls_acc), label='Train Accuracy')\n",
    "    plt.plot(range(epoch), np.array(val_cls_acc), label='Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    # plt.xticks\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Test Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(save_fig_acc)\n",
    "\n",
    "def checkpoint(valid_class_acc, \n",
    "               val_total_loss,\n",
    "               old_valid_class_acc,\n",
    "               old_valid_loss,\n",
    "               epoch, \n",
    "               model,\n",
    "               optimizer,\n",
    "               check_folder\n",
    "#                    logs\n",
    "              ):\n",
    "\n",
    "    if valid_class_acc >= old_valid_class_acc and val_total_loss <= old_valid_loss:\n",
    "        old_valid_class_acc = valid_class_acc\n",
    "        old_valid_loss = val_total_loss\n",
    "        save_dict = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': val_total_loss,\n",
    "            'test_acc': valid_class_acc\n",
    "        }\n",
    "\n",
    "     # Saving best model\n",
    "        now = datetime.now().strftime(\"%m-%d-%Y - %H-%M-%S\")\n",
    "        run_dir = os.getcwd() + f\"/{check_folder}\"\n",
    "        if not os.path.exists(run_dir):\n",
    "            os.mkdir(run_dir)\n",
    "    #         save_dir = run_dir + f\"/{now}\"\n",
    "    #         if not os.path.exists(save_dir):\n",
    "    #             os.mkdir(save_dir)\n",
    "        save_best_model_dir = run_dir + \"/save_best_model\"\n",
    "        if not os.path.exists(save_best_model_dir):\n",
    "            os.mkdir(save_best_model_dir)\n",
    "        save_best_model_path = save_best_model_dir + f\"/{save_dict['loss']:>7f}_{save_dict['test_acc']:>7f}_{now}.pt\"\n",
    "        torch.save(save_dict, save_best_model_path)\n",
    "        \n",
    "def classification_report_csv(report, auc, check_folder):\n",
    "    now = datetime.now().strftime(\"%m-%d-%Y - %H-%M-%S\")\n",
    "    run_dir = save_dir + f\"/{check_folder}\"\n",
    "    if not os.path.exists(run_dir):\n",
    "        os.mkdir(run_dir)\n",
    "    save_report_dir = run_dir + \"/save_classification_report\"\n",
    "    if not os.path.exists(save_report_dir):\n",
    "        os.mkdir(save_report_dir)\n",
    "        \n",
    "    report_data = report['macro avg']\n",
    "    del report_data['support']\n",
    "    report_data.update({'auc': auc})\n",
    "#     print(type(report_data))\n",
    "#     print(report_data)\n",
    "\n",
    "#     dataframe = pd.DataFrame.from_dict(report_data, orient='index')\n",
    "#     save_report_file = save_report_dir + f\"/classification_report_{now}.csv\"\n",
    "#     dataframe.to_csv(save_report_file, index = False)\n",
    "    with open(save_report_dir + f\"/cls_report_{now}.json\", \"w\") as outfile: \n",
    "        json.dump(report_data, outfile)\n",
    "    \n",
    "def acc_loss_json(log_dict, check_folder):\n",
    "    now = datetime.now().strftime(\"%m-%d-%Y - %H-%M-%S\")\n",
    "    run_dir = save_dir + f\"/{check_folder}\"\n",
    "    if not os.path.exists(run_dir):\n",
    "        os.mkdir(run_dir)\n",
    "    save_json_dir = run_dir + \"/save_acc_loss_json\"\n",
    "    if not os.path.exists(save_json_dir):\n",
    "        os.mkdir(save_json_dir) \n",
    "    save_acc_loss_file = save_json_dir + f\"/acc_loss_{now}.json\"\n",
    "    with open(save_acc_loss_file, \"w\") as outfile: \n",
    "        json.dump(log_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9ca150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict = {\n",
    "    \"train\": {\n",
    "        \"acc\": [],\n",
    "        \"loss\": []\n",
    "    },\n",
    "    \"valid\": {\n",
    "        \"acc\": [],\n",
    "        \"loss\": []\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8989c016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [00:12, 11.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: nan - train acc: 4.027712541099108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1065it [00:10, 97.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: nan - valid acc: 6.009389671361502\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:04,  9.57it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m train_label \u001b[38;5;241m=\u001b[39m train_label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# print(train_sig)\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_sig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# print(pred)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# loss = focalloss_fn(pred, train_label)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, train_label)\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[21], line 15\u001b[0m, in \u001b[0;36mHeartModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):        \n\u001b[0;32m---> 15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mori_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/torchvision/models/efficientnet.py:343\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/torchvision/models/efficientnet.py:333\u001b[0m, in \u001b[0;36mEfficientNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 333\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[1;32m    336\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/torchvision/models/efficientnet.py:164\u001b[0m, in \u001b[0;36mMBConv.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 164\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_res_connect:\n\u001b[1;32m    166\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstochastic_depth(result)\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2452\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "# class_la = np.unique(label_df[\"Rhythm\"].values.tolist())\n",
    "class_la = []\n",
    "for i in range (11):\n",
    "    class_la.append(i+1)\n",
    "for i in range (len(class_la)):\n",
    "    class_la[i] = str(class_la[i])\n",
    "train_losses = []\n",
    "train_acc_plot = []\n",
    "val_losses = []\n",
    "val_acc_plot = []\n",
    "old_valid_class_acc = 0\n",
    "old_valid_loss = 1e23\n",
    "for e in range(epoch):\n",
    "    model.train()\n",
    "    print(f\"Epoch: {e+1}\")\n",
    "    batch_cnt = 0\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for batch, (train_sig, train_label) in tqdm(enumerate(traindl)):\n",
    "        batch_cnt = batch\n",
    "        train_sig = train_sig.to(device)\n",
    "        train_label = train_label.to(device)\n",
    "        # print(train_sig)\n",
    "        \n",
    "        pred = model(train_sig)\n",
    "        # print(pred)\n",
    "        \n",
    "        # loss = focalloss_fn(pred, train_label)\n",
    "        loss = loss_fn(pred, train_label)\n",
    "        # print(train_label)\n",
    "        # print(pred)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == train_label).type(torch.float).sum().item()\n",
    "        \n",
    "    total_loss /= batch_cnt\n",
    "    correct /= len(traindl.dataset)\n",
    "    # log_dict[\"train\"][\"loss\"].append(total_loss)\n",
    "    # log_dict[\"train\"][\"acc\"].append(correct)\n",
    "    \n",
    "    print(f\"train loss: {total_loss} - train acc: {100*correct}\")\n",
    "\n",
    "# Valid\n",
    "    batch_cnt = 0\n",
    "    val_total_loss = 0\n",
    "    val_correct = 0\n",
    "    model.eval()\n",
    "    y_true_list = [] \n",
    "    pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch, (valid_sig, valid_label) in tqdm(enumerate(validdl)):\n",
    "            batch_cnt = batch\n",
    "            valid_label = valid_label.to(device)\n",
    "            \n",
    "            valid_sig = valid_sig.to(device)  \n",
    "            pred = model(valid_sig)\n",
    "            pred_pos = pred.argmax(1)\n",
    "            \n",
    "            y_true_list.append(valid_label)\n",
    "            pred_list.append(pred_pos)\n",
    "            \n",
    "            loss = loss_fn(pred, valid_label)\n",
    "            val_total_loss += loss.item()\n",
    "            val_correct += (pred.argmax(1) == valid_label).type(torch.float).sum().item()\n",
    "            \n",
    "        val_total_loss /= batch_cnt\n",
    "        val_correct /= len(validdl.dataset)\n",
    "        # log_dict[\"valid\"][\"loss\"].append(val_total_loss)\n",
    "        # log_dict[\"valid\"][\"acc\"].append(val_correct)\n",
    "\n",
    "        if val_correct > best_acc:\n",
    "            best_acc = val_correct\n",
    "            best_ep = e \n",
    "\n",
    "        print(f\"valid loss: {val_total_loss} - valid acc: {100*val_correct}\")\n",
    "#         checkpoint(valid_class_acc = val_correct, \n",
    "#                    val_total_loss = val_total_loss,\n",
    "#                    old_valid_class_acc = old_valid_class_acc ,\n",
    "#                    old_valid_loss = old_valid_loss,\n",
    "#                    epoch = e, \n",
    "#                    model = model,\n",
    "#                    optimizer = optimizer,\n",
    "#                    check_folder = checkpoint_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04d253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(reports.keys())\n",
    "# print(reports['macro avg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df57c6c",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
