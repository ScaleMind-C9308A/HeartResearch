{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6169743",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "038e91c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acaa0560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7302e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import os, sys, shutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from glob import glob\n",
    "import random\n",
    "import cv2 as cv\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80737247-a198-4481-bb11-c775cce8918f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/mountHDD2/linh/git/HeartResearch/Chapman Dataset/Reimplement\n",
      "['Reimplement', 'Data_Explore', 'Diagnostics.xlsx', '.ipynb_checkpoints', '2D_model', '1D_Model', '2D_Loss']\n",
      "/media/mountHDD2/linh/git/HeartResearch/Chapman Dataset/Reimplement\n"
     ]
    }
   ],
   "source": [
    "# main_data_dir =  \"/media/mountHDD2\"\n",
    "print(os.getcwd())\n",
    "save_dir = os.getcwd()\n",
    "os.chdir(\"..\")\n",
    "# os.chdir(\"..\")\n",
    "main_dir = os.getcwd() \n",
    "print(os.listdir(main_dir))\n",
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "822504e0-fad3-48b2-95d3-0d5f51c2a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/media/mountHDD3/data_storage/biomedical_data/ecg_data/ECGDataDenoised\"\n",
    "label_file = main_dir + \"/Diagnostics.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ca748a3-5b7a-4253-864d-e054185288c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MUSE    10588\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = []\n",
    "for file in glob(data_dir + \"/*\"):\n",
    "    ls.append(file.split(\"/\")[-1].split(\".\")[0].split(\"_\")[0])\n",
    "ls_list = pd.Series(ls)\n",
    "ls_list.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec1aa643-c6d4-47e2-8e5f-9740dcbf985b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName</th>\n",
       "      <th>Rhythm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MUSE_20180120_121711_19000</td>\n",
       "      <td>SA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MUSE_20180120_121704_86000</td>\n",
       "      <td>SA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MUSE_20180113_125357_13000</td>\n",
       "      <td>SA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MUSE_20180113_134825_04000</td>\n",
       "      <td>SB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MUSE_20180115_123455_79000</td>\n",
       "      <td>SB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10641</th>\n",
       "      <td>MUSE_20181222_204246_47000</td>\n",
       "      <td>SVT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10642</th>\n",
       "      <td>MUSE_20180115_120332_79000</td>\n",
       "      <td>SA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10643</th>\n",
       "      <td>MUSE_20180712_152507_30000</td>\n",
       "      <td>AF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10644</th>\n",
       "      <td>MUSE_20180118_181350_17000</td>\n",
       "      <td>SA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10645</th>\n",
       "      <td>MUSE_20180116_121646_28000</td>\n",
       "      <td>ST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10646 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         FileName Rhythm\n",
       "0      MUSE_20180120_121711_19000     SA\n",
       "1      MUSE_20180120_121704_86000     SA\n",
       "2      MUSE_20180113_125357_13000     SA\n",
       "3      MUSE_20180113_134825_04000     SB\n",
       "4      MUSE_20180115_123455_79000     SB\n",
       "...                           ...    ...\n",
       "10641  MUSE_20181222_204246_47000    SVT\n",
       "10642  MUSE_20180115_120332_79000     SA\n",
       "10643  MUSE_20180712_152507_30000     AF\n",
       "10644  MUSE_20180118_181350_17000     SA\n",
       "10645  MUSE_20180116_121646_28000     ST\n",
       "\n",
       "[10646 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag_df = pd.read_excel(label_file)\n",
    "label_df = diag_df[['FileName', 'Rhythm']]\n",
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6ff0d7f-b131-4c96-9924-5c651e45017c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName</th>\n",
       "      <th>VentricularRate</th>\n",
       "      <th>AtrialRate</th>\n",
       "      <th>QRSDuration</th>\n",
       "      <th>QTInterval</th>\n",
       "      <th>QTCorrected</th>\n",
       "      <th>RAxis</th>\n",
       "      <th>TAxis</th>\n",
       "      <th>QRSCount</th>\n",
       "      <th>QOnset</th>\n",
       "      <th>QOffset</th>\n",
       "      <th>TOffset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MUSE_20180120_121711_19000</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>80</td>\n",
       "      <td>366</td>\n",
       "      <td>414</td>\n",
       "      <td>59</td>\n",
       "      <td>28</td>\n",
       "      <td>13</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MUSE_20180120_121704_86000</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>80</td>\n",
       "      <td>402</td>\n",
       "      <td>427</td>\n",
       "      <td>29</td>\n",
       "      <td>53</td>\n",
       "      <td>11</td>\n",
       "      <td>220</td>\n",
       "      <td>260</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MUSE_20180113_125357_13000</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>86</td>\n",
       "      <td>334</td>\n",
       "      <td>417</td>\n",
       "      <td>86</td>\n",
       "      <td>65</td>\n",
       "      <td>15</td>\n",
       "      <td>213</td>\n",
       "      <td>256</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MUSE_20180113_134825_04000</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>102</td>\n",
       "      <td>432</td>\n",
       "      <td>424</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>211</td>\n",
       "      <td>262</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MUSE_20180115_123455_79000</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>84</td>\n",
       "      <td>424</td>\n",
       "      <td>419</td>\n",
       "      <td>55</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>221</td>\n",
       "      <td>263</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10641</th>\n",
       "      <td>MUSE_20181222_204246_47000</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>234</td>\n",
       "      <td>354</td>\n",
       "      <td>619</td>\n",
       "      <td>178</td>\n",
       "      <td>144</td>\n",
       "      <td>30</td>\n",
       "      <td>159</td>\n",
       "      <td>276</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10642</th>\n",
       "      <td>MUSE_20180115_120332_79000</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>76</td>\n",
       "      <td>400</td>\n",
       "      <td>428</td>\n",
       "      <td>77</td>\n",
       "      <td>63</td>\n",
       "      <td>12</td>\n",
       "      <td>219</td>\n",
       "      <td>257</td>\n",
       "      <td>419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10643</th>\n",
       "      <td>MUSE_20180712_152507_30000</td>\n",
       "      <td>102</td>\n",
       "      <td>340</td>\n",
       "      <td>90</td>\n",
       "      <td>398</td>\n",
       "      <td>518</td>\n",
       "      <td>48</td>\n",
       "      <td>210</td>\n",
       "      <td>17</td>\n",
       "      <td>211</td>\n",
       "      <td>256</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10644</th>\n",
       "      <td>MUSE_20180118_181350_17000</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>102</td>\n",
       "      <td>398</td>\n",
       "      <td>450</td>\n",
       "      <td>-16</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>210</td>\n",
       "      <td>261</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10645</th>\n",
       "      <td>MUSE_20180116_121646_28000</td>\n",
       "      <td>101</td>\n",
       "      <td>101</td>\n",
       "      <td>104</td>\n",
       "      <td>334</td>\n",
       "      <td>433</td>\n",
       "      <td>85</td>\n",
       "      <td>42</td>\n",
       "      <td>17</td>\n",
       "      <td>213</td>\n",
       "      <td>265</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10646 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         FileName  VentricularRate  AtrialRate  QRSDuration  \\\n",
       "0      MUSE_20180120_121711_19000               77          77           80   \n",
       "1      MUSE_20180120_121704_86000               68          68           80   \n",
       "2      MUSE_20180113_125357_13000               94          94           86   \n",
       "3      MUSE_20180113_134825_04000               58          58          102   \n",
       "4      MUSE_20180115_123455_79000               59          59           84   \n",
       "...                           ...              ...         ...          ...   \n",
       "10641  MUSE_20181222_204246_47000              184           0          234   \n",
       "10642  MUSE_20180115_120332_79000               69          69           76   \n",
       "10643  MUSE_20180712_152507_30000              102         340           90   \n",
       "10644  MUSE_20180118_181350_17000               77          77          102   \n",
       "10645  MUSE_20180116_121646_28000              101         101          104   \n",
       "\n",
       "       QTInterval  QTCorrected  RAxis  TAxis  QRSCount  QOnset  QOffset  \\\n",
       "0             366          414     59     28        13     211      251   \n",
       "1             402          427     29     53        11     220      260   \n",
       "2             334          417     86     65        15     213      256   \n",
       "3             432          424      2     60        10     211      262   \n",
       "4             424          419     55     44         9     221      263   \n",
       "...           ...          ...    ...    ...       ...     ...      ...   \n",
       "10641         354          619    178    144        30     159      276   \n",
       "10642         400          428     77     63        12     219      257   \n",
       "10643         398          518     48    210        17     211      256   \n",
       "10644         398          450    -16     30        13     210      261   \n",
       "10645         334          433     85     42        17     213      265   \n",
       "\n",
       "       TOffset  \n",
       "0          394  \n",
       "1          421  \n",
       "2          380  \n",
       "3          427  \n",
       "4          433  \n",
       "...        ...  \n",
       "10641      336  \n",
       "10642      419  \n",
       "10643      410  \n",
       "10644      409  \n",
       "10645      380  \n",
       "\n",
       "[10646 rows x 12 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attribute_df = diag_df.iloc[:, 5:]\n",
    "attribute_df = diag_df[['FileName', 'VentricularRate', 'AtrialRate', 'QRSDuration', 'QTInterval',\n",
    "                       'QTCorrected', 'RAxis', 'TAxis', 'QRSCount', 'QOnset', 'QOffset', 'TOffset']]\n",
    "attribute_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f95eb86-077a-47e2-93dd-7c78a87ccb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AF' 'AFIB' 'AT' 'AVNRT' 'AVRT' 'SA' 'SAAWR' 'SB' 'SR' 'ST' 'SVT']\n"
     ]
    }
   ],
   "source": [
    "unique_values = np.unique(label_df[\"Rhythm\"].values.tolist())\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "026fe8bf-feda-41f6-9922-d08f6ae82028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_paths = []\n",
    "# for file in glob(data_dir +\"/*\"):\n",
    "#     data_paths.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57741b9a-62d6-4045-b75b-2db44888a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict = {\n",
    "#     idx : [] for idx in unique_values\n",
    "# }\n",
    "\n",
    "# for data_path in data_paths:\n",
    "#     filename = data_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    \n",
    "#     # Get the corresponding 'Rhythm' values for the filename\n",
    "#     rhythm_values = label_df[label_df[\"FileName\"] == filename][\"Rhythm\"].values\n",
    "    \n",
    "#     if rhythm_values.size == 1:\n",
    "#         _cls = rhythm_values.item()  # Safe to use .item() if there's exactly one match\n",
    "#     elif rhythm_values.size > 1:\n",
    "#         _cls = rhythm_values[0]  # Take the first element if there are multiple matches\n",
    "#     else:\n",
    "#         continue  # Skip if no matches are found\n",
    "    \n",
    "#     data_dict[_cls].append(data_path)\n",
    "\n",
    "# for key in data_dict:\n",
    "#     print(f\"{key}->{len(data_dict[key])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad2806b3-c6ba-47ba-9713-6ed3a2657d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         FileName  Rhythm\n",
      "0      MUSE_20180120_121711_19000       5\n",
      "1      MUSE_20180120_121704_86000       5\n",
      "2      MUSE_20180113_125357_13000       5\n",
      "3      MUSE_20180113_134825_04000       7\n",
      "4      MUSE_20180115_123455_79000       7\n",
      "...                           ...     ...\n",
      "10641  MUSE_20181222_204246_47000      10\n",
      "10642  MUSE_20180115_120332_79000       5\n",
      "10643  MUSE_20180712_152507_30000       0\n",
      "10644  MUSE_20180118_181350_17000       5\n",
      "10645  MUSE_20180116_121646_28000       9\n",
      "\n",
      "[10646 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_353957/787631178.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  label_df['Rhythm'] = label_encoder.fit_transform(label_df['Rhythm'])\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_df['Rhythm'] = label_encoder.fit_transform(label_df['Rhythm'])\n",
    "print(label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "551f03a0-8686-42da-8ab5-76c1c41b477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = label_df[\"FileName\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22419d37-c440-4859-9763-f9fdd9906d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "unique_values = np.unique(label_df[\"Rhythm\"].values.tolist())\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb86b7df-ea31-4428-9b40-d24fe4e3b018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10588\n"
     ]
    }
   ],
   "source": [
    "data_paths = []\n",
    "for file in glob(data_dir +\"/*\"):\n",
    "    data_paths.append(file)\n",
    "print(len(data_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10056b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8470\n",
      "8470\n"
     ]
    }
   ],
   "source": [
    "ratio = [0.8, 0.1]\n",
    "\n",
    "train_index = int(len(data_paths)*ratio[0])\n",
    "valid_index = int(len(data_paths)*(ratio[0]+ratio[1]))\n",
    "print(train_index)\n",
    "train_mat_paths = data_paths[:train_index]\n",
    "valid_mat_paths = data_paths[train_index:valid_index]\n",
    "print(len(train_mat_paths))\n",
    "\n",
    "train_label = label_df.iloc[:train_index,:]\n",
    "valid_label = label_df.iloc[train_index:valid_index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "864b1ccf-2b0a-4197-b739-39a1cb009ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8470\n",
      "1059\n"
     ]
    }
   ],
   "source": [
    "print(len(train_label))\n",
    "print(len(valid_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f84c9a40-20c6-4943-a86b-9f42ef0ada45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, ..., 0, 5, 9])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df[label_df[\"FileName\"] == filenames][\"Rhythm\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6950ff1-537e-4cc6-b742-4b4748e504b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 77  77  80 366 414  59  28  13 211 251 394]\n"
     ]
    }
   ],
   "source": [
    "ECG_feature = attribute_df[attribute_df[\"FileName\"] == 'MUSE_20180120_121711_19000'].iloc[:, 1:].to_numpy()[0]\n",
    "print(ECG_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189b94c0",
   "metadata": {},
   "source": [
    " # Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39c47eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.nn.functional import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c33309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []\n",
    "class HeartData(Dataset):\n",
    "    def __init__(self, data_paths, label_df, attribute_df):\n",
    "        self.data_paths = data_paths\n",
    "        random.shuffle(self.data_paths)\n",
    "        self.label_df = label_df\n",
    "        self.attribute_df = attribute_df\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224), antialias=None),\n",
    "            normalize\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_path = self.data_paths[idx]        \n",
    "        # data = loadmat(data_path)['ECG'][0][0][2]\n",
    "        data = pd.read_csv(data_path, header = None)\n",
    "        data = data.values.T\n",
    "        clip_data = data[:, 500:3000]\n",
    "        clip_data = torch.tensor(clip_data, dtype=torch.float32)\n",
    "        # normalized_data = (clip_data - clip_data.min()) / (clip_data.max() - clip_data.min())\n",
    "        # grayscale_images = (normalized_data * 255)\n",
    "        # grayscale_images = grayscale_images.unsqueeze(0).unsqueeze(0) # (1, 1, h, w)\n",
    "        # resized_images = F.interpolate(grayscale_images, size=(9*4,2500), mode='bilinear', align_corners=True)\n",
    "        # resized_images = resized_images.squeeze(0).squeeze(0)\n",
    "        # torch_data = resized_images.unsqueeze(0).repeat(3, 1, 1)\n",
    "        # torch_data_resize = self.transform(torch_data)\n",
    "\n",
    "        filename = data_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        label = self.label_df[self.label_df[\"FileName\"] == filename][\"Rhythm\"].values.item()\n",
    "        ECG_feature = self.attribute_df[self.attribute_df[\"FileName\"] == filename].iloc[:, 1:].to_numpy()[0]\n",
    "\n",
    "        return clip_data, label, ECG_feature\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7adad047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1059\n"
     ]
    }
   ],
   "source": [
    "train_ds = HeartData(train_mat_paths, label_df, attribute_df)\n",
    "valid_ds = HeartData(valid_mat_paths, label_df, attribute_df)\n",
    "print(len(valid_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75847351",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "batch_size = 64\n",
    "\n",
    "traindl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    pin_memory=True, \n",
    "    num_workers=os.cpu_count()//2\n",
    ")\n",
    "\n",
    "validdl = DataLoader(\n",
    "    valid_ds,\n",
    "    batch_size=29, \n",
    "    # shuffle=True, \n",
    "    pin_memory=True, \n",
    "    num_workers=os.cpu_count()//2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad0e825",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce830560-6387-4e36-a1e4-1a00b0182b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        # Input size: (batch_size, 12, 300)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(12, 64, kernel_size=21, stride=11)\n",
    "        # self.pca1 = nn.Linear(64,11)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "         \n",
    "\n",
    "        self.conv6 = nn.Conv1d(64, 128, kernel_size=7, stride=1)\n",
    "        # self.pca6 = nn.Linear(128,11)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Conv1d(128, 256, kernel_size=5, stride=1),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        \n",
    "        self.conv11 = nn.Conv1d(256, 512, kernel_size=13, stride=1)\n",
    "        # self.pca11 = nn.Linear(512,11)\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(512, 256, kernel_size=7, stride=1),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.conv14 = nn.Conv1d(256, 256, kernel_size=9, stride=1)\n",
    "        self.pca14 = nn.Linear(256,11)\n",
    "        self.pool4 = nn.MaxPool1d(2)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=256, hidden_size=128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, 11)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: (batch_size, 12, 300)\n",
    "        x = self.conv1(x)  # (batch_size, 64, ?)\n",
    "        x = self.layer1(x)  # (batch_size, 64, ?)\n",
    "    \n",
    "        x = self.conv6(x)  # (batch_size, 128, ?)\n",
    "        x = self.layer2(x)  # (batch_size, 256, ?)\n",
    "    \n",
    "        x = self.conv11(x)  # (batch_size, 512, ?)\n",
    "        x = self.layer3(x)  # (batch_size, 256, ?)\n",
    "    \n",
    "        x_feature = self.conv14(x)  # (batch_size, 256, ?)\n",
    "        x = self.pool4(x_feature)\n",
    "    \n",
    "        # Flatten for Linear layer\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, seq_len, 256)\n",
    "        # pca14 = x.reshape(-1, 256)  # (batch_size * seq_len, 256)\n",
    "        # pca14 = self.pca14(x)  # Linear layer: (batch_size * seq_len, 11)\n",
    "        \n",
    "        # # Reshape back to batch format (if needed for downstream tasks)\n",
    "        # seq_len = pca14.size(0) // x.size(0)  # Compute seq_len\n",
    "        # pca14 = pca14.view(-1, seq_len, 11)  # (batch_size, seq_len, 11)\n",
    "    \n",
    "        # Use pooled feature maps\n",
    "        # x = self.pool4(x)  # If further processing is required\n",
    "        # x = x.permute(0, 2, 1)  # (batch_size, seq_len, 256)\n",
    "    \n",
    "        x, _ = self.lstm(x)  # LSTM expects (batch_size, seq_len, input_size)\n",
    "        x = x[:, -1, :]  # Take the last time step's output\n",
    "        out = self.fc(x)  # Final output layer\n",
    "        return out, x_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b807d76-283e-4cf1-8107-c5b385b4e81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 10])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1,12,2500)\n",
    "out_DNN, feature = DNN()(a)\n",
    "print(feature.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe4995",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f66b6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 2\n",
    "lr = 0.001 # lr = 0.001 Acc: 0.821875 lr = 0.0005 Acc: 0.825 Focal_loss Acc: 0.828\n",
    "best_acc = 0\n",
    "best_ep = 0\n",
    "\n",
    "model = DNN()\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "scheduler = CosineAnnealingLR(optimizer=optimizer, T_max=epoch*len(traindl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c23f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalClassifierV0(nn.Module):\n",
    "    def __init__(self, gamma=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.act = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "\n",
    "        logits = self.act(pred)\n",
    "\n",
    "        B, C = tuple(logits.size())\n",
    "\n",
    "        entropy = torch.pow(1 - logits, self.gamma) * logits * F.one_hot(target, num_classes=C).float()\n",
    "\n",
    "        return (-1 / B) * torch.sum(entropy)\n",
    "\n",
    "focalloss_fn = FocalClassifierV0()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# checkpoint_folder = \"run_efficientB0_heatmap_gamma0.5_lr0.0005\"\n",
    "checkpoint_folder = \"run_deep_feature_0.3_0.01_lr00005\"\n",
    "# checkpoint_folder = \"run_proposed_gamma5_0.01_lr0001_10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4725f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_plot(train_losses, val_losses, n_epochs, check_folder):\n",
    "    now = datetime.now().strftime(\"%m-%d-%Y - %H-%M-%S\")\n",
    "    run_dir = save_dir + f\"/{check_folder}\"\n",
    "    if not os.path.exists(run_dir):\n",
    "        os.mkdir(run_dir)\n",
    "    save_loss_dir = run_dir + \"/save_losses\"\n",
    "    if not os.path.exists(save_loss_dir):\n",
    "        os.mkdir(save_loss_dir)\n",
    "    save_fig_losses = os.path.join(save_loss_dir, f\"plot_losses_epoch{n_epochs}_{now}.png\")  \n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(epoch), np.array(train_losses), label='Train Loss')\n",
    "    plt.plot(range(epoch), np.array(val_losses), label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    # plt.xticks()\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Test Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(save_fig_losses)\n",
    "    \n",
    "\n",
    "def acc_plot(train_cls_acc, val_cls_acc, n_epochs, check_folder):\n",
    "    now = datetime.now().strftime(\"%m-%d-%Y - %H-%M-%S\")\n",
    "    run_dir = save_dir + f\"/{check_folder}\"\n",
    "    if not os.path.exists(run_dir):\n",
    "        os.mkdir(run_dir)\n",
    "#         save_dir = run_dir + f\"/{now}\"\n",
    "#         if not os.path.exists(save_dir):\n",
    "#             os.mkdir(save_dir)\n",
    "    save_acc_dir = run_dir + \"/save_acc\"\n",
    "    if not os.path.exists(save_acc_dir):\n",
    "        os.mkdir(save_acc_dir)\n",
    "    save_fig_acc = os.path.join(save_acc_dir, 'plot_acc_epoch{}_{}.png'.format(n_epochs, now))  \n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(epoch), np.array(train_cls_acc), label='Train Accuracy')\n",
    "    plt.plot(range(epoch), np.array(val_cls_acc), label='Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    # plt.xticks\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Test Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(save_fig_acc)\n",
    "\n",
    "def checkpoint(valid_class_acc, \n",
    "               val_total_loss,\n",
    "               old_valid_class_acc,\n",
    "               old_valid_loss,\n",
    "               epoch, \n",
    "               model,\n",
    "               optimizer,\n",
    "               check_folder\n",
    "#                    logs\n",
    "              ):\n",
    "\n",
    "    if valid_class_acc >= old_valid_class_acc and val_total_loss <= old_valid_loss:\n",
    "        old_valid_class_acc = valid_class_acc\n",
    "        old_valid_loss = val_total_loss\n",
    "        save_dict = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': val_total_loss,\n",
    "            'test_acc': valid_class_acc\n",
    "        }\n",
    "\n",
    "     # Saving best model\n",
    "        now = datetime.now().strftime(\"%m-%d-%Y - %H-%M-%S\")\n",
    "        run_dir = save_dir + f\"/{check_folder}\"\n",
    "        if not os.path.exists(run_dir):\n",
    "            os.mkdir(run_dir)\n",
    "    #         save_dir = run_dir + f\"/{now}\"\n",
    "    #         if not os.path.exists(save_dir):\n",
    "    #             os.mkdir(save_dir)\n",
    "        save_best_model_dir = run_dir + \"/save_best_model\"\n",
    "        if not os.path.exists(save_best_model_dir):\n",
    "            os.mkdir(save_best_model_dir)\n",
    "        save_best_model_path = save_best_model_dir + f\"/{save_dict['loss']:>7f}_{save_dict['test_acc']:>7f}_{now}.pt\"\n",
    "        torch.save(save_dict, save_best_model_path)\n",
    "        \n",
    "def classification_report_csv(report, auc, check_folder):\n",
    "    now = datetime.now().strftime(\"%m-%d-%Y - %H-%M-%S\")\n",
    "    run_dir = save_dir + f\"/{check_folder}\"\n",
    "    if not os.path.exists(run_dir):\n",
    "        os.mkdir(run_dir)\n",
    "    save_report_dir = run_dir + \"/save_classification_report\"\n",
    "    if not os.path.exists(save_report_dir):\n",
    "        os.mkdir(save_report_dir)\n",
    "        \n",
    "    report_data = report['macro avg']\n",
    "    del report_data['support']\n",
    "    report_data.update({'auc': auc})\n",
    "#     print(type(report_data))\n",
    "#     print(report_data)\n",
    "\n",
    "#     dataframe = pd.DataFrame.from_dict(report_data, orient='index')\n",
    "#     save_report_file = save_report_dir + f\"/classification_report_{now}.csv\"\n",
    "#     dataframe.to_csv(save_report_file, index = False)\n",
    "    with open(save_report_dir + f\"/cls_report_{now}.json\", \"w\") as outfile: \n",
    "        json.dump(report_data, outfile)\n",
    "    \n",
    "def acc_loss_json(log_dict, check_folder):\n",
    "    now = datetime.now().strftime(\"%m-%d-%Y - %H-%M-%S\")\n",
    "    run_dir = save_dir + f\"/{check_folder}\"\n",
    "    if not os.path.exists(run_dir):\n",
    "        os.mkdir(run_dir)\n",
    "    save_json_dir = run_dir + \"/save_acc_loss_json\"\n",
    "    if not os.path.exists(save_json_dir):\n",
    "        os.mkdir(save_json_dir) \n",
    "    save_acc_loss_file = save_json_dir + f\"/acc_loss_{now}.json\"\n",
    "    with open(save_acc_loss_file, \"w\") as outfile: \n",
    "        json.dump(log_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9ca150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict = {\n",
    "    \"train\": {\n",
    "        \"acc\": [],\n",
    "        \"loss\": []\n",
    "    },\n",
    "    \"valid\": {\n",
    "        \"acc\": [],\n",
    "        \"loss\": []\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "600f6e4d-7c0b-4c2b-ae7b-d55a5f1e782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "class_la = []\n",
    "for i in range (11):\n",
    "    class_la.append(i)\n",
    "for i in range (len(class_la)):\n",
    "    class_la[i] = str(class_la[i])\n",
    "train_losses = []\n",
    "train_acc_plot = []\n",
    "val_losses = []\n",
    "val_acc_plot = []\n",
    "old_valid_class_acc = 0\n",
    "old_valid_loss = 1e23\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "pca = PCA(n_components=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b91803b4-4f6a-4be7-8882-44e2548e1363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "133it [00:03, 37.16it/s]\n",
      "37it [00:00, 62.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "133it [00:07, 18.48it/s]\n",
      "1it [00:00,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:00, 13.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:00, 33.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:00, 50.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(29, 256, 10)\n",
      "(29, 2560)\n",
      "(15, 256, 10)\n",
      "(15, 2560)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:01, 29.58it/s]\n"
     ]
    }
   ],
   "source": [
    "train_feature_list = []\n",
    "train_label_list = []\n",
    "for e in range(epoch):\n",
    "    model.train()\n",
    "    print(f\"Epoch: {e+1}\")\n",
    "    batch_cnt = 0\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for batch, (train_sig, train_label, feature_train) in tqdm(enumerate(traindl)):\n",
    "        \n",
    "        batch_cnt = batch\n",
    "        train_sig = train_sig.to(device)\n",
    "        # print(train_sig.shape)\n",
    "        train_label = train_label.to(device)\n",
    "        # print(train_label.shape)\n",
    "\n",
    "        pred, feature_map_train = model(train_sig)\n",
    "        X_signal_train_features = feature_map_train.cpu().detach().numpy()\n",
    "        if e == 1:\n",
    "            X_signal_train_pca = pca.fit_transform(X_signal_train_features.reshape(X_signal_train_features.shape[0], -1))\n",
    "            # print(X_signal_train_pca.shape)\n",
    "            X_train_combined = torch.cat((torch.tensor(X_signal_train_pca), feature_train), dim=1).cpu().detach().numpy()\n",
    "            # print(X_train_combined.shape)\n",
    "            train_feature_list.extend(X_train_combined)\n",
    "            train_label_list.extend(train_label.cpu())\n",
    "        loss = loss_fn(pred, train_label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    test_feature_list = []\n",
    "    test_label_list = []\n",
    "    batch_cnt = 0\n",
    "    val_total_loss = 0\n",
    "    val_correct = 0\n",
    "    accuracy = 0\n",
    "    model.eval()\n",
    "    y_true_list = [] \n",
    "    pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch, (valid_sig, valid_label, feature_test) in tqdm(enumerate(validdl)):\n",
    "            batch_cnt = batch\n",
    "            valid_label = valid_label.to(device)\n",
    "            valid_sig = valid_sig.to(device)  \n",
    "            pred_test, feature_map_test = model(valid_sig)\n",
    "            X_signal_test_features = feature_map_test.cpu().detach().numpy()\n",
    "            # y_true_list.extent(valid_label)\n",
    "            # X_signal_train_pca = pca.fit_transform(X_signal_train_features.reshape(X_signal_train_features.shape[0], -1))\n",
    "            if e == 1:\n",
    "                print(X_signal_test_features.shape)\n",
    "                print(X_signal_test_features.reshape(X_signal_test_features.shape[0], -1).shape)\n",
    "                X_signal_test_pca = pca.fit_transform(X_signal_test_features.reshape(X_signal_test_features.shape[0], -1))\n",
    "                \n",
    "            # X_train_combined = np.hstack([X_signal_train_pca, feature_train])\n",
    "            # X_test_combined = np.hstack([X_signal_test_pca, feature_test])\n",
    "            # X_train_combined = torch.cat(( torch.tensor(X_signal_train_pca), feature_train), dim=0)\n",
    "                X_test_combined = torch.cat(( torch.tensor(X_signal_test_pca), feature_test), dim=1).cpu().detach().numpy()\n",
    "                test_feature_list.extend(X_test_combined)\n",
    "                test_label_list.extend(valid_label.cpu())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3532d20b-1b20-4fbb-9723-3bed8bc2edf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8470\n",
      "8470\n"
     ]
    }
   ],
   "source": [
    "print(len(train_feature_list))\n",
    "print(len(train_label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b18f542-1c8e-459d-a003-c0627449d1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 85.08%\n",
      "f1: 45.24%\n"
     ]
    }
   ],
   "source": [
    "rf_model.fit(train_feature_list, train_label_list)\n",
    "        \n",
    "        # Evaluate the model\n",
    "y_pred = rf_model.predict(test_feature_list)\n",
    "# print(y_pred)\n",
    "# pred_list.append(y_pred)\n",
    "accuracy = accuracy_score(test_label_list, y_pred)\n",
    "f1 = f1_score(test_label_list, y_pred, average = \"macro\")\n",
    "print(f\"Random Forest Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"f1: {f1 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04d253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(reports.keys())\n",
    "# print(reports['macro avg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df57c6c",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
