{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6169743",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "038e91c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7302e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import os, sys, shutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "# from tqdm import tqdm, trange\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82a9d0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/linhpika/git/HeartResearch/Experiment/Approach/EfficientB0\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f58c040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/linhpika/git/HeartResearch\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9ae6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425820\n"
     ]
    }
   ],
   "source": [
    "\n",
    "main_data_dir = os.getcwd() + \"/Experiment\"\n",
    "\n",
    "label_csv_path = main_data_dir + \"/Label_img\"\n",
    "\n",
    "# lead = ['I','II','III','aVR','aVL','aVF','V1','V2','V3','V4','V5','V6']\n",
    "lead = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "lead_to_onehot = {leads : torch.nn.functional.one_hot(torch.tensor([index])[0], num_classes = 12) for index, leads in enumerate(lead)}\n",
    "\n",
    "class_la = [1,2,3,4,5,6,7,8,9]\n",
    "class_to_onehot = {classes : torch.nn.functional.one_hot(torch.tensor([index])[0], num_classes = 9) for index, classes in enumerate(class_la)}\n",
    "\n",
    "img_data_dir = \"/media/mountHDD1/ecg/med_scaleogram_h256_w512_seglen1600_scl500\"\n",
    "img_data_list =  glob(img_data_dir + \"/*\")\n",
    "\n",
    "print(len(os.listdir(img_data_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d0e6dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lead_to_onehot[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1c533f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/mountHDD1/ecg/med_scaleogram_h256_w512_seglen1600_scl500/A0118_lead7_seg3.png\n"
     ]
    }
   ],
   "source": [
    "print(img_data_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "237a8291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Image</th>\n",
       "      <th>Class</th>\n",
       "      <th>Lead</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A0118_lead7_seg3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A3997_lead9_seg5</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A2161_lead2_seg7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A4685_lead1_seg13</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A1999_lead0_seg4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0              Image  Class  Lead\n",
       "0           0   A0118_lead7_seg3      3     7\n",
       "1           1   A3997_lead9_seg5      7     9\n",
       "2           2   A2161_lead2_seg7      5     2\n",
       "3           3  A4685_lead1_seg13      5     1\n",
       "4           4   A1999_lead0_seg4      6     0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df = pd.read_csv(label_csv_path)\n",
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "825bb663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df[\"Class\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189b94c0",
   "metadata": {},
   "source": [
    " # Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39c47eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.nn.functional import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6be9ad2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "667ff10e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340656\n",
      "42582\n",
      "42582\n",
      "(340656, 4)\n",
      "(42582, 4)\n",
      "(42582, 4)\n"
     ]
    }
   ],
   "source": [
    "ratio = [0.8, 0.1, 0.1]\n",
    "\n",
    "train_index = int(len(img_data_list)*ratio[0])\n",
    "valid_index = int(len(img_data_list)*(ratio[0]+ratio[1]))\n",
    "\n",
    "train_image_paths = img_data_list[:train_index]\n",
    "valid_image_paths = img_data_list[train_index:valid_index]\n",
    "test_image_paths = img_data_list[valid_index:]\n",
    "\n",
    "train_label = label_df.iloc[:train_index,:]\n",
    "valid_label = label_df.iloc[train_index:valid_index,:]\n",
    "test_label = label_df.iloc[valid_index:,:]\n",
    "\n",
    "print(len(train_image_paths))\n",
    "print(len(valid_image_paths))\n",
    "print(len(test_image_paths))\n",
    "\n",
    "print(train_label.shape)\n",
    "print(valid_label.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5324d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/mountHDD1/ecg/med_scaleogram_h256_w512_seglen1600_scl500/A0118_lead7_seg3.png'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "476c6e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Image</th>\n",
       "      <th>Class</th>\n",
       "      <th>Lead</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A0118_lead7_seg3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A3997_lead9_seg5</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A2161_lead2_seg7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A4685_lead1_seg13</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A1999_lead0_seg4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340651</th>\n",
       "      <td>340651</td>\n",
       "      <td>A5061_lead4_seg3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340652</th>\n",
       "      <td>340652</td>\n",
       "      <td>A6644_lead3_seg2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340653</th>\n",
       "      <td>340653</td>\n",
       "      <td>A3898_lead10_seg9</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340654</th>\n",
       "      <td>340654</td>\n",
       "      <td>A5733_lead4_seg6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340655</th>\n",
       "      <td>340655</td>\n",
       "      <td>A6812_lead0_seg2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340656 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0              Image  Class  Lead\n",
       "0                0   A0118_lead7_seg3      3     7\n",
       "1                1   A3997_lead9_seg5      7     9\n",
       "2                2   A2161_lead2_seg7      5     2\n",
       "3                3  A4685_lead1_seg13      5     1\n",
       "4                4   A1999_lead0_seg4      6     0\n",
       "...            ...                ...    ...   ...\n",
       "340651      340651   A5061_lead4_seg3      5     4\n",
       "340652      340652   A6644_lead3_seg2      3     3\n",
       "340653      340653  A3898_lead10_seg9      1    10\n",
       "340654      340654   A5733_lead4_seg6      5     4\n",
       "340655      340655   A6812_lead0_seg2      5     0\n",
       "\n",
       "[340656 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "047a15b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = torch.nn.Sequential(transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)))\n",
    "\n",
    "class HeartData(Dataset):\n",
    "    def __init__(self, label_df, data_path):\n",
    "        self.label_df = label_df\n",
    "        self.data_path = data_path\n",
    "        \n",
    "#         self.onehot_label_class = one_hot(self.label_df['Class'])\n",
    "#         self.onehot_label_lead = one_hot(self.label_df['Lead'])\n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "#         imgs = []\n",
    "#         labels = []\n",
    "#         labels.append(self.onehot_label_class)\n",
    "#         labels.append(self.onehot_label_lead)\n",
    "        class_label = self.label_df['Class'][index]\n",
    "        class_label = class_to_onehot[class_label]\n",
    "    \n",
    "        data_img = cv.imread(self.data_path[index])\n",
    "        torch_img = torch.from_numpy(data_img).permute(-1, 0, 1).float()\n",
    "        torch_img = normalize(torch_img)\n",
    "        \n",
    "#         lead_label = self.onehot_label_lead[index]\n",
    "#         class_label = self.onehot_label_class[index]\n",
    "        \n",
    "        return class_label, torch_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3cd41b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 197, 396])\n"
     ]
    }
   ],
   "source": [
    "class_label = train_label['Class'][0]\n",
    "class_label = class_to_onehot[class_label]\n",
    "print(type(class_label))\n",
    "data_img = cv.imread(train_image_paths[1])\n",
    "torch_img = torch.from_numpy(data_img).permute(-1, 0, 1)\n",
    "print(type(torch_img))\n",
    "print(torch_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5b6f513",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HeartData(label_df, train_image_paths)\n",
    "valid_dataset = HeartData(label_df, valid_image_paths)\n",
    "test_dataset = HeartData(label_df, test_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43f2e5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42582"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32e7734c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0, 0, 0, 1, 0]),\n",
       " tensor([[[ 556.8340,  556.8340,  556.8340,  ...,  556.8340,  556.8340,\n",
       "            556.8340],\n",
       "          [ 556.8340,  556.8340,  556.8340,  ...,  574.3013,  714.0393,\n",
       "            792.6419],\n",
       "          [ 556.8340,  556.8340,  556.8340,  ...,  596.1354,  635.4366,\n",
       "            814.4760],\n",
       "          ...,\n",
       "          [1107.0524, 1080.8516, 1093.9519,  ..., 1093.9519,  696.5720,\n",
       "            954.2140],\n",
       "          [1093.9519, 1093.9519, 1111.4192,  ..., 1080.8516,  727.1397,\n",
       "           1107.0524],\n",
       "          [1080.8516, 1093.9519, 1111.4192,  ..., 1093.9519,  783.9083,\n",
       "           1063.3843]],\n",
       " \n",
       "         [[  -2.0357,   -2.0357,   -2.0357,  ...,   -2.0357,   -2.0357,\n",
       "             -2.0357],\n",
       "          [  -2.0357,   -2.0357,   -2.0357,  ...,   -2.0357,   -2.0357,\n",
       "             -2.0357],\n",
       "          [  -2.0357,   -2.0357,   -2.0357,  ...,   -2.0357,   -2.0357,\n",
       "             -2.0357],\n",
       "          ...,\n",
       "          [ 984.5714, 1020.2857, 1002.4286,  ..., 1002.4286, 1136.3572,\n",
       "           1136.3572],\n",
       "          [1002.4286, 1002.4286,  948.8571,  ..., 1020.2857, 1136.3572,\n",
       "            984.5714],\n",
       "          [1020.2857, 1002.4286,  877.4286,  ..., 1002.4286, 1136.3572,\n",
       "           1033.6786]],\n",
       " \n",
       "         [[  -1.8044,   -1.8044,   -1.8044,  ...,   -1.8044,   -1.8044,\n",
       "             -1.8044],\n",
       "          [  -1.8044,   -1.8044,   -1.8044,  ...,   -1.8044,   -1.8044,\n",
       "             -1.8044],\n",
       "          [  -1.8044,   -1.8044,   -1.8044,  ...,   -1.8044,   -1.8044,\n",
       "             -1.8044],\n",
       "          ...,\n",
       "          [  -1.8044,   -1.8044,   -1.8044,  ...,   -1.8044,  380.4178,\n",
       "            122.6400],\n",
       "          [  -1.8044,   -1.8044,   -1.8044,  ...,   -1.8044,  353.7511,\n",
       "             -1.8044],\n",
       "          [  -1.8044,   -1.8044,   -1.8044,  ...,   -1.8044,  295.9734,\n",
       "              7.0844]]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "474cc4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size = 64, shuffle = True, pin_memory = True, num_workers = 48)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size = 64, shuffle = True, pin_memory = True, num_workers = 48)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 64, shuffle = True, pin_memory = True, num_workers = 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba6674a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5323"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44078ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-task learning - efficient B0\n",
    "# data loader -> 2 labels: lead + disease\n",
    "# Model: 2 output: 1 vector for 12 leads (softmax) + 1 vector for disease\n",
    "# loss funct: loss lead + loss class => backward\n",
    "# random choice: notice: seed(python, numpy, torch) same\n",
    "        \n",
    "# multi-channel - efficient B2 - quite similar to video classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad0e825",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "651dfeb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from torch import nn\n",
    "\n",
    "ori_model = efficientnet_b0(weights = EfficientNet_B0_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29d7764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ori_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09e59aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeartModel(\n",
      "  (ori_model): EfficientNet(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (2): Conv2dNormActivation(\n",
      "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
      "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
      "        )\n",
      "        (2): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
      "        )\n",
      "        (2): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
      "        )\n",
      "        (2): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
      "        )\n",
      "        (3): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (8): Conv2dNormActivation(\n",
      "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=1280, out_features=9, bias=True)\n",
      "      (2): Softmax(dim=1)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from torch import nn\n",
    "\n",
    "class HeartModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ori_model = efficientnet_b0(weights = EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        del self.ori_model.classifier\n",
    "        self.ori_model.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1280, 9),\n",
    "            nn.Softmax(dim = 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        logits = self.ori_model(x)\n",
    "        return (logits)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HeartModel().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe4995",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8989c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "opt_mapping = {\n",
    "    \"Adam\" : torch.optim.Adam\n",
    "}\n",
    "\n",
    "loss_mapping = {\n",
    "    \"CCE\" : nn.CrossEntropyLoss()\n",
    "}\n",
    "\n",
    "class Training:\n",
    "    def __init__(self, \n",
    "                 device: str = \"cpu\",\n",
    "                 learning_rate:float = 0.0001,\n",
    "                 optimizer:str = \"Adam\",\n",
    "                 loss:str = \"CCE\",\n",
    "                 model = model,\n",
    "#                  batchsize:int = 32,\n",
    "                 epochs:int = 100,\n",
    "#                  label_df: pd.DataFrame = label_df, \n",
    "#                  root_dir: str = img_data_dir, \n",
    "#                  ratio: tuple = (0.8, 0.1, 0.1),\n",
    "#                  resize: tuple = None, #  (256, 512)\n",
    "#                  seed: int = 777\n",
    "                ):\n",
    "        \n",
    "        # Setup\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        self.model.to(self.device)\n",
    "        self.lr = learning_rate\n",
    "        self.optimizer = opt_mapping[optimizer](self.model.parameters(), lr=self.lr)\n",
    "        self.loss_fn = loss_mapping[loss]\n",
    "#         self.bs = batchsize\n",
    "        self.ep = epochs\n",
    "#         self.default_ratio = (0.001, 0.001, 0.001)\n",
    "        self.cls_accuracy = Accuracy(task=\"multiclass\", num_classes=9).to(self.device)\n",
    "#         self.target_names = [str(i) for i in range(9)]\n",
    "        \n",
    "        # Data\n",
    "        self.train_data = train_dataloader\n",
    "        self.valid_data = valid_dataloader\n",
    "        self.test_data = test_dataloader\n",
    "\n",
    "    \n",
    "    def __update__(self):\n",
    "        train_losses = []\n",
    "        train_cls_acc_plot = []\n",
    "        \n",
    "        val_losses = []\n",
    "        val_cls_acc_plot = []\n",
    "        \n",
    "        for e in range(self.ep):\n",
    "            train_class_acc = 0\n",
    "            batch_cnt = 0\n",
    "            self.model.train()\n",
    "            for batch, (y, X) in tqdm(enumerate(self.train_data)):\n",
    "                batch_cnt = batch\n",
    "                y = y.to(self.device)\n",
    "                pred = self.model((X/255).to(self.device))\n",
    "                train_loss = self.loss_fn(pred, y.to(self.device, dtype = torch.float))\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                train_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_class_acc += self.cls_accuracy(torch.argmax(pred, dim = 1), torch.argmax(y, dim = 1)).item()\n",
    "                        \n",
    "            mean_train_cls_acc = train_class_acc/(batch_cnt + 1)\n",
    "  \n",
    "            train_losses.append(train_loss)\n",
    "            train_cls_acc_plot.append(mean_train_cls_acc)\n",
    "\n",
    "            print(f\"Epoch: {e} - Train Loss: {train_loss.item()} - Train class acc: {mean_train_cls_acc}\")\n",
    "            \n",
    "            val_loss, mean_valid_cls_acc = self.validation(epoch = e)\n",
    "            val_losses.append(val_loss)\n",
    "            val_cls_acc_plot.append(mean_valid_cls_acc)\n",
    "            # Show train_loss, train_acc, val_loss, val_acc\n",
    "            if e%9 == 0: \n",
    "                self.loss_plot(train_losses = train_losses, \n",
    "                          val_losses = val_losses, n_epochs = e + 1, \n",
    "                          check_folder = 'run_efficientB0')\n",
    "                self.cls_acc_plot(train_cls_acc = train_cls_acc_plot , val_cls_acc = val_cls_acc_plot, n_epochs = e+1, check_folder = 'run_efficientB0')\n",
    "               \n",
    "        self.evaluation()\n",
    "        \n",
    "    def validation(self, epoch):\n",
    "        \n",
    "        self.model.eval()\n",
    "        valid_class_acc = 0\n",
    "        batch_cnt = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            valid_class_acc = 0\n",
    "            valid_lead_acc = 0\n",
    "            batch_cnt = 0\n",
    "            for batch, (y, X) in tqdm(enumerate(self.valid_data)):\n",
    "                batch_cnt = batch\n",
    "\n",
    "                y = y.to(self.device)\n",
    "                pred = self.model((X/255).to(self.device))\n",
    "\n",
    "                val_loss = self.loss_fn(pred, y.to(self.device, dtype = torch.float))\n",
    "\n",
    "                valid_class_acc += self.cls_accuracy(torch.argmax(pred, dim = 1), torch.argmax(y, dim = 1)).item()                \n",
    "\n",
    "            mean_valid_cls_acc = valid_class_acc/(batch_cnt + 1)\n",
    "            \n",
    "            self.checkpoint(valid_class_acc = mean_valid_cls_acc, \n",
    "                       val_total_loss = val_loss,\n",
    "                       epoch = self.ep, \n",
    "                       model = self.model,\n",
    "                       optimizer = self.optimizer,\n",
    "                       check_folder = 'run_efficientB0')\n",
    "\n",
    "            print(f\"Epoch: {epoch} - Val_loss: {val_loss.item()} - Val class acc: {mean_valid_cls_acc}\")\n",
    "        \n",
    "        return val_loss, mean_valid_cls_acc\n",
    "\n",
    "    def loss_plot(self, train_losses, val_losses, n_epochs, check_folder):\n",
    "        now = datetime.now().strftime(\"%m-%d-%Y - %H-%M-%S\")\n",
    "        run_dir = os.getcwd() + f\"/{check_folder}\"\n",
    "        if not os.path.exists(run_dir):\n",
    "            os.mkdir(run_dir)\n",
    "            \n",
    "#         save_dir = run_dir + f\"/{now}\"\n",
    "#         if not os.path.exists(save_dir):\n",
    "#             os.mkdir(save_dir)\n",
    "            \n",
    "        save_loss_dir = run_dir + \"/save_losses\"\n",
    "        if not os.path.exists(save_loss_dir):\n",
    "            os.mkdir(save_loss_dir)\n",
    "        save_fig_losses = os.path.join(save_loss_dir, f\"plot_losses_epoch{n_epochs}_{now}.png\")  \n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(range(1, n_epochs + 1), torch.tensor(train_losses).cpu(), label='Train Loss')\n",
    "        plt.plot(range(1, n_epochs + 1), torch.tensor(val_losses).cpu(), label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        # plt.xticks()\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig(save_fig_losses)\n",
    "        \n",
    "    def cls_acc_plot(self, train_cls_acc, val_cls_acc, n_epochs, check_folder):\n",
    "        now = datetime.now().strftime(\"%m-%d-%Y - %H-%M-%S\")\n",
    "        run_dir = os.getcwd() + f\"/{check_folder}\"\n",
    "        if not os.path.exists(run_dir):\n",
    "            os.mkdir(run_dir)\n",
    "#         save_dir = run_dir + f\"/{now}\"\n",
    "#         if not os.path.exists(save_dir):\n",
    "#             os.mkdir(save_dir)\n",
    "        save_acc_dir = run_dir + \"/save_acc\"\n",
    "        if not os.path.exists(save_acc_dir):\n",
    "            os.mkdir(save_acc_dir)\n",
    "        save_fig_acc = os.path.join(save_acc_dir, 'plot_cls_acc_epoch{}_{}.png'.format(n_epochs, now))  \n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(range(1, n_epochs + 1), torch.tensor(train_cls_acc).cpu(), label='Train Accuracy')\n",
    "        plt.plot(range(1, n_epochs + 1), torch.tensor(val_cls_acc).cpu(), label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        # plt.xticks\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.legend()\n",
    "        plt.savefig(save_fig_acc)\n",
    "\n",
    "    \n",
    "    def checkpoint(self, valid_class_acc, \n",
    "                   val_total_loss,\n",
    "                   epoch, \n",
    "                   model,\n",
    "                   optimizer,\n",
    "                   check_folder\n",
    "#                    logs\n",
    "                  ):\n",
    "        old_valid_class_acc = 0\n",
    "        old_valid_loss = 1e23\n",
    "        if valid_class_acc >= old_valid_class_acc and val_total_loss <= old_valid_loss:\n",
    "            old_valid_class_acc = valid_class_acc\n",
    "            old_valid_loss = val_total_loss\n",
    "            save_dict = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': val_total_loss,\n",
    "                'val_class_acc': valid_class_acc\n",
    "            }\n",
    "            \n",
    "         # Saving best model\n",
    "            now = datetime.now().strftime(\"%m-%d-%Y - %H-%M-%S\")\n",
    "            run_dir = os.getcwd() + f\"/{check_folder}\"\n",
    "            if not os.path.exists(run_dir):\n",
    "                os.mkdir(run_dir)\n",
    "        #         save_dir = run_dir + f\"/{now}\"\n",
    "        #         if not os.path.exists(save_dir):\n",
    "        #             os.mkdir(save_dir)\n",
    "            save_best_model_dir = run_dir + \"/save_best_model\"\n",
    "            if not os.path.exists(save_best_model_dir):\n",
    "                os.mkdir(save_best_model_dir)\n",
    "            save_best_model_path = save_best_model_dir + f\"/{save_dict['loss']:>7f}_{save_dict['val_class_acc']:>7f}_{now}.pt\"\n",
    "            torch.save(save_dict, save_best_model_path)\n",
    "    \n",
    "    def evaluation(self):\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        y_eval = [] \n",
    "        pred_eval = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch, (y, X) in tqdm(enumerate(self.test_data)):\n",
    "                batch_cnt = batch\n",
    "                \n",
    "                y = y.to(self.device)\n",
    "                pred = self.model((X/255).to(self.device))\n",
    "                \n",
    "                y_eval.append(torch.argmax(y, dim=1)[0])\n",
    "                pred_eval.append(torch.argmax(pred, dim=1)[0])\n",
    "                \n",
    "            y_eval = torch.stack(y_eval)\n",
    "            pred_eval = torch.stack(pred_eval)\n",
    "                \n",
    "            pred = torch.nn.functional.one_hot(pred_eval, num_classes = 9)\n",
    "            y = torch.nn.functional.one_hot(y1_eval, num_classes = 9)\n",
    "\n",
    "            \n",
    "            # Classification report\n",
    "            for i in range (len(class_la)):\n",
    "                class_la[i] = str(class_la[i])\n",
    "\n",
    "            # Classification report\n",
    "            print(classification_report(\n",
    "                torch.argmax(y, dim=1).cpu().numpy().tolist(), \n",
    "                torch.argmax(pred, dim=1).cpu().numpy().tolist(),\n",
    "                target_names=class_la))    \n",
    "\n",
    "            # Confusion Matrix\n",
    "            ConfusionMatrixDisplay.from_predictions(torch.argmax(y, dim=1).cpu().numpy().tolist(), torch.argmax(pred, dim=1).cpu().numpy().tolist(), cmap = \"PuBuGn\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "            # ROC Curve\n",
    "\n",
    "            for i in range(9):\n",
    "                r2 = roc_auc_score(y[:, i].cpu(), pred[:, i].cpu())\n",
    "                print(\"The ROC AUC score of \"+ class_la[i] +\" is: \"+str(r2))\n",
    "\n",
    "\n",
    "            # Compute ROC curve and ROC area for each class\n",
    "            class_fpr = {}\n",
    "            class_tpr = {}\n",
    "            class_roc_auc = dict()\n",
    "            for i in range(9):\n",
    "                class_fpr[i], class_tpr[i], _ = roc_curve(y[:, i].cpu(), pred[:, i].cpu(), drop_intermediate=False)\n",
    "                roc_auc[i] = auc(class_fpr[i], class_tpr[i])\n",
    "\n",
    "            plt.plot(class_fpr[0], class_tpr[0],'turquoise',label='1: ROC curve of class 1 (area = %0.2f)' % roc_auc[0])\n",
    "            plt.plot(class_fpr[1], class_tpr[1],'peachpuff',label='2: ROC curve of class 2 (area = %0.2f)' % roc_auc[1])\n",
    "            plt.plot(class_fpr[2], class_tpr[2],'paleturquoise',label='3: ROC curve of class 3 (area = %0.2f)' % roc_auc[2])\n",
    "            plt.plot(class_fpr[3], class_tpr[3],'pink',label='4: ROC curve of class 4 (area = %0.2f)' % roc_auc[3])\n",
    "            plt.plot(class_fpr[4], class_tpr[4],'lightcoral',label='5: ROC curve of class 5 (area = %0.2f)' % roc_auc[4])\n",
    "            plt.plot(class_fpr[5], class_tpr[5],'peachpuff',label='6: ROC curve of class 6 (area = %0.2f)' % roc_auc[5])\n",
    "            plt.plot(class_fpr[6], class_tpr[6],'steelblue',label='7: ROC curve of class 7 (area = %0.2f)' % roc_auc[6])\n",
    "            plt.plot(class_fpr[7], class_tpr[7],'forestgreen',label='8: ROC curve of class 8 (area = %0.2f)' % roc_auc[6])\n",
    "            plt.plot(class_fpr[8], class_tpr[8],'darkslategray',label='9: ROC curve of class 9 (area = %0.2f)' % roc_auc[6])\n",
    "\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.xlim([-0.1, 1.1])\n",
    "            plt.ylim([-0.1, 1.1])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('Receiver operating characteristic of lead')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.show()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafd1b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5323it [13:04,  6.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - Train Loss: 1.791873812675476 - Train class acc: 0.5830486019751014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "666it [00:32, 20.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - Val_loss: 2.073786973953247 - Val class acc: 0.13960338180308585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1168it [02:52,  6.84it/s]"
     ]
    }
   ],
   "source": [
    "# Metadata\n",
    "\n",
    "# Training Setup\n",
    "device = \"cuda\"\n",
    "learning_rate = 0.0001\n",
    "optimizer = \"Adam\"\n",
    "loss = \"CCE\"\n",
    "# batchsize = 128\n",
    "epochs = 100\n",
    "# ratio = (0.8, 0.1, 0.1)\n",
    "\n",
    "# Data\n",
    "\n",
    "monitor = Training(device = device,\n",
    "                   learning_rate = learning_rate,\n",
    "                   optimizer = optimizer,\n",
    "                   loss = loss,\n",
    "                   model = model,\n",
    "#                    batchsize = batchsize,\n",
    "                   epochs = epochs)\n",
    "#                    label_df = label_df, \n",
    "#                    root_dir = img_data_dir, \n",
    "#                    ratio = ratio)\n",
    "monitor.__update__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d181ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unitest\n",
    "\n",
    "# monitor.get_sample_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c51f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df57c6c",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b71e558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor.evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cbcfa8",
   "metadata": {},
   "source": [
    "# Clear Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9747ed99",
   "metadata": {},
   "source": [
    "# monitor.clear_buffer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
