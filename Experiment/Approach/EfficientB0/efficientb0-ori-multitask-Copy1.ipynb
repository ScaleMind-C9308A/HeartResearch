{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6169743",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "038e91c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7302e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import os, sys, shutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "# from tqdm import tqdm, trange\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82a9d0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/linhpika/git/HeartResearch/Experiment/Approach/EfficientB0\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f58c040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/linhpika/git/HeartResearch\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c9ae6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425820\n"
     ]
    }
   ],
   "source": [
    "# D:\\GitCloneProject\\HeartResearch\\Data set\\v4_data\\med_scaleogram_h256_w512_seglen1600_scl500\n",
    "main_data_dir = os.getcwd() + \"/Data set\"\n",
    "\n",
    "label_csv_path = main_data_dir + \"/Label_img\"\n",
    "\n",
    "# lead = ['I','II','III','aVR','aVL','aVF','V1','V2','V3','V4','V5','V6']\n",
    "lead = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "lead_to_onehot = {leads : torch.nn.functional.one_hot(torch.tensor([index])[0], num_classes = 12) for index, leads in enumerate(lead)}\n",
    "\n",
    "class_la = [1,2,3,4,5,6,7,8,9]\n",
    "class_to_onehot = {classes : torch.nn.functional.one_hot(torch.tensor([index])[0], num_classes = 9) for index, classes in enumerate(class_la)}\n",
    "\n",
    "img_data_dir = \"/media/mountHDD1/ecg/med_scaleogram_h256_w512_seglen1600_scl500\"\n",
    "img_data_list =  glob(img_data_dir + \"/*\")\n",
    "\n",
    "print(len(os.listdir(img_data_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1c533f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/mountHDD1/ecg/med_scaleogram_h256_w512_seglen1600_scl500/A0118_lead7_seg3.png\n"
     ]
    }
   ],
   "source": [
    "print(img_data_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "237a8291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Image</th>\n",
       "      <th>Class</th>\n",
       "      <th>Lead</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A0118_lead7_seg3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A3997_lead9_seg5</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A2161_lead2_seg7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A4685_lead1_seg13</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A1999_lead0_seg4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0              Image  Class  Lead\n",
       "0           0   A0118_lead7_seg3      3     7\n",
       "1           1   A3997_lead9_seg5      7     9\n",
       "2           2   A2161_lead2_seg7      5     2\n",
       "3           3  A4685_lead1_seg13      5     1\n",
       "4           4   A1999_lead0_seg4      6     0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df = pd.read_csv(label_csv_path)\n",
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "825bb663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df[\"Class\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb39c9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Label.csv', 'README.md', 'Label_img']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(main_data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189b94c0",
   "metadata": {},
   "source": [
    " # Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39c47eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.nn.functional import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6be9ad2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "667ff10e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340656\n",
      "42582\n",
      "42582\n",
      "(340656, 4)\n",
      "(42582, 4)\n",
      "(42582, 4)\n"
     ]
    }
   ],
   "source": [
    "ratio = [0.8, 0.1, 0.1]\n",
    "\n",
    "train_index = int(len(img_data_list)*ratio[0])\n",
    "valid_index = int(len(img_data_list)*(ratio[0]+ratio[1]))\n",
    "\n",
    "train_image_paths = img_data_list[:train_index]\n",
    "valid_image_paths = img_data_list[train_index:valid_index]\n",
    "test_image_paths = img_data_list[valid_index:]\n",
    "\n",
    "train_label = label_df.iloc[:train_index,:]\n",
    "valid_label = label_df.iloc[train_index:valid_index,:]\n",
    "test_label = label_df.iloc[valid_index:,:]\n",
    "\n",
    "print(len(train_image_paths))\n",
    "print(len(valid_image_paths))\n",
    "print(len(test_image_paths))\n",
    "\n",
    "print(train_label.shape)\n",
    "print(valid_label.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5324d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/mountHDD1/ecg/med_scaleogram_h256_w512_seglen1600_scl500/A0118_lead7_seg3.png'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "476c6e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Image</th>\n",
       "      <th>Class</th>\n",
       "      <th>Lead</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A0118_lead7_seg3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A3997_lead9_seg5</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A2161_lead2_seg7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A4685_lead1_seg13</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A1999_lead0_seg4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340651</th>\n",
       "      <td>340651</td>\n",
       "      <td>A5061_lead4_seg3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340652</th>\n",
       "      <td>340652</td>\n",
       "      <td>A6644_lead3_seg2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340653</th>\n",
       "      <td>340653</td>\n",
       "      <td>A3898_lead10_seg9</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340654</th>\n",
       "      <td>340654</td>\n",
       "      <td>A5733_lead4_seg6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340655</th>\n",
       "      <td>340655</td>\n",
       "      <td>A6812_lead0_seg2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340656 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0              Image  Class  Lead\n",
       "0                0   A0118_lead7_seg3      3     7\n",
       "1                1   A3997_lead9_seg5      7     9\n",
       "2                2   A2161_lead2_seg7      5     2\n",
       "3                3  A4685_lead1_seg13      5     1\n",
       "4                4   A1999_lead0_seg4      6     0\n",
       "...            ...                ...    ...   ...\n",
       "340651      340651   A5061_lead4_seg3      5     4\n",
       "340652      340652   A6644_lead3_seg2      3     3\n",
       "340653      340653  A3898_lead10_seg9      1    10\n",
       "340654      340654   A5733_lead4_seg6      5     4\n",
       "340655      340655   A6812_lead0_seg2      5     0\n",
       "\n",
       "[340656 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "047a15b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeartData(Dataset):\n",
    "    def __init__(self, label_df, data_path):\n",
    "        self.label_df = label_df\n",
    "        self.data_path = data_path\n",
    "        \n",
    "#         self.onehot_label_class = one_hot(self.label_df['Class'])\n",
    "#         self.onehot_label_lead = one_hot(self.label_df['Lead'])\n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "#         imgs = []\n",
    "#         labels = []\n",
    "#         labels.append(self.onehot_label_class)\n",
    "#         labels.append(self.onehot_label_lead)\n",
    "        class_label = self.label_df['Class'][index]\n",
    "        class_label = class_to_onehot[class_label]\n",
    "    \n",
    "        lead_label = self.label_df['Lead'][index]\n",
    "        lead_label = lead_to_onehot[lead_label]\n",
    "\n",
    "        data_img = cv.imread(self.data_path[index])\n",
    "        torch_img = torch.from_numpy(data_img).permute(-1, 0, 1)\n",
    "        \n",
    "#         lead_label = self.onehot_label_lead[index]\n",
    "#         class_label = self.onehot_label_class[index]\n",
    "        \n",
    "        return lead_label, class_label, torch_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3cd41b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 253, 506])\n"
     ]
    }
   ],
   "source": [
    "class_label = train_label['Class'][0]\n",
    "class_label = class_to_onehot[class_label]\n",
    "print(type(class_label))\n",
    "lead_label = train_label['Lead'][0]\n",
    "lead_label = lead_to_onehot[lead_label]\n",
    "print(type(lead_label))\n",
    "data_img = cv.imread(train_image_paths[1])\n",
    "torch_img = torch.from_numpy(data_img).permute(-1, 0, 1)\n",
    "print(type(torch_img))\n",
    "print(torch_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5b6f513",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HeartData(label_df, train_image_paths)\n",
    "valid_dataset = HeartData(label_df, valid_image_paths)\n",
    "test_dataset = HeartData(label_df, test_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43f2e5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42582"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32e7734c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 1, 0]),\n",
       " tensor([[[128, 128, 128,  ..., 128, 128, 128],\n",
       "          [128, 128, 128,  ..., 132, 164, 182],\n",
       "          [128, 128, 128,  ..., 137, 146, 187],\n",
       "          ...,\n",
       "          [254, 248, 251,  ..., 251, 160, 219],\n",
       "          [251, 251, 255,  ..., 248, 167, 254],\n",
       "          [248, 251, 255,  ..., 251, 180, 244]],\n",
       " \n",
       "         [[  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          ...,\n",
       "          [221, 229, 225,  ..., 225, 255, 255],\n",
       "          [225, 225, 213,  ..., 229, 255, 221],\n",
       "          [229, 225, 197,  ..., 225, 255, 232]],\n",
       " \n",
       "         [[  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,  86,  28],\n",
       "          [  0,   0,   0,  ...,   0,  80,   0],\n",
       "          [  0,   0,   0,  ...,   0,  67,   2]]], dtype=torch.uint8))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset[5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "474cc4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size = 16, shuffle = True, pin_memory = True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size = 16, shuffle = True, pin_memory = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 1, shuffle = True, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba6674a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1098"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44078ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-task learning - efficient B0\n",
    "# data loader -> 2 labels: lead + disease\n",
    "# Model: 2 output: 1 vector for 12 leads (softmax) + 1 vector for disease\n",
    "# loss funct: loss lead + loss class => backward\n",
    "# random choice: notice: seed(python, numpy, torch) same\n",
    "        \n",
    "# multi-channel - efficient B2 - quite similar to video classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad0e825",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "651dfeb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-3dd342df.pth\" to /home/linhpika/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-3dd342df.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from torch import nn\n",
    "\n",
    "ori_model = efficientnet_b0(weights = EfficientNet_B0_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29d7764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ori_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09e59aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeartModel(\n",
      "  (ori_model): EfficientNet(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (2): Conv2dNormActivation(\n",
      "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
      "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
      "        )\n",
      "        (2): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
      "        )\n",
      "        (2): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
      "        )\n",
      "        (2): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
      "        )\n",
      "        (3): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (8): Conv2dNormActivation(\n",
      "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=1280, out_features=21, bias=True)\n",
      "      (2): Softmax(dim=1)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from torch import nn\n",
    "\n",
    "class HeartModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ori_model = efficientnet_b0(weights = EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        del self.ori_model.classifier\n",
    "        self.ori_model.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1280, 21),\n",
    "            nn.Softmax(dim = 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        logits = self.ori_model(x)\n",
    "        \n",
    "        return (logits[:, :12], logits[:, 12:])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HeartModel().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe4995",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8989c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "opt_mapping = {\n",
    "    \"Adam\" : torch.optim.Adam\n",
    "}\n",
    "\n",
    "loss_mapping = {\n",
    "    \"CCE\" : nn.CrossEntropyLoss()\n",
    "}\n",
    "\n",
    "class Training:\n",
    "    def __init__(self, \n",
    "                 device: str = \"cpu\",\n",
    "                 learning_rate:float = 0.0001,\n",
    "                 optimizer:str = \"Adam\",\n",
    "                 loss:str = \"CCE\",\n",
    "                 model = model,\n",
    "#                  batchsize:int = 32,\n",
    "                 epochs:int = 100,\n",
    "#                  label_df: pd.DataFrame = label_df, \n",
    "#                  root_dir: str = img_data_dir, \n",
    "#                  ratio: tuple = (0.8, 0.1, 0.1),\n",
    "#                  resize: tuple = None, #  (256, 512)\n",
    "#                  seed: int = 777\n",
    "                ):\n",
    "        \n",
    "        # Setup\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        self.model.to(self.device)\n",
    "        self.lr = learning_rate\n",
    "        self.optimizer = opt_mapping[optimizer](self.model.parameters(), lr=self.lr)\n",
    "        self.loss_fn = loss_mapping[loss]\n",
    "        self.ep = epochs\n",
    "        self.lead_accuracy = Accuracy(task=\"multiclass\", num_classes=12).to(self.device)\n",
    "        self.cls_accuracy = Accuracy(task=\"multiclass\", num_classes=9).to(self.device)\n",
    "        \n",
    "        # Data\n",
    "        self.train_data = train_dataloader\n",
    "        self.valid_data = valid_dataloader\n",
    "        self.test_data = test_dataloader\n",
    "    \n",
    "\n",
    "    \n",
    "    def __update__(self):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        for e in range(self.ep):\n",
    "            train_class_acc = 0\n",
    "            train_lead_acc = 0\n",
    "            batch_cnt = 0\n",
    "            self.model.train()\n",
    "            for batch, (y1, y2, X) in tqdm(enumerate(self.train_data)):\n",
    "                batch_cnt = batch\n",
    "                y1, y2 = y1.to(self.device), y2.to(self.device)\n",
    "                pred1, pred2 = self.model((X/255).to(self.device))\n",
    "                l1 = self.loss_fn(pred1, y1.to(self.device, dtype = torch.float))\n",
    "                l2 = self.loss_fn(pred2, y2.to(self.device, dtype = torch.float))\n",
    "                train_loss = l1 + l2\n",
    "\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                train_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_lead_acc += self.lead_accuracy(torch.argmax(pred1, dim = 1), torch.argmax(y1, dim = 1)).item()\n",
    "                train_class_acc += self.cls_accuracy(torch.argmax(pred2, dim = 1), torch.argmax(y2, dim = 1)).item()\n",
    "                        \n",
    "            mean_train_cls_acc = train_class_acc/(batch_cnt + 1)\n",
    "            mean_train_led_acc = train_lead_acc/(batch_cnt + 1)\n",
    "            # Show train_loss, train_acc, val_loss, val_acc\n",
    "            if e%10 == 0: \n",
    "                print(f\"Epoch: {e} - Train Loss: {train_loss.item()} - Train class acc: {mean_train_cls_acc} - Train lead acc: {mean_train_led_acc}\")\n",
    "#             \" - Val Loss: {val_loss.item()} - Val Acc: {val_acc.item()}\")\n",
    "            train_losses.append(train_loss)\n",
    "            self.validation()\n",
    "        loss_plot(self, train_losses = train_losses, val_losses = val_losses, n_epochs = e + 1, check_folder = 'run_efficientB0')\n",
    "        self.evaluation()\n",
    "        \n",
    "    def validation(self):\n",
    "        self.model.eval()\n",
    "        valid_class_acc = 0\n",
    "        valid_lead_acc = 0\n",
    "        batch_cnt = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            valid_class_acc = 0\n",
    "            valid_lead_acc = 0\n",
    "            batch_cnt = 0\n",
    "            for batch, (y1, y2, X) in tqdm(enumerate(self.valid_data)):\n",
    "                batch_cnt = batch\n",
    "\n",
    "                y1, y2 = y1.to(self.device), y2.to(self.device)\n",
    "                pred1, pred2 = self.model((X/255).to(self.device))\n",
    "\n",
    "                l1 = self.loss_fn(pred1, y1.to(self.device, dtype = torch.float))\n",
    "                l2 = self.loss_fn(pred2, y2.to(self.device, dtype = torch.float))\n",
    "                val_loss = l1 + l2\n",
    "                \n",
    "\n",
    "                valid_lead_acc += self.lead_accuracy(torch.argmax(pred1, dim = 1), torch.argmax(y1, dim = 1)).item()\n",
    "                valid_class_acc += self.cls_accuracy(torch.argmax(pred2, dim = 1), torch.argmax(y2, dim = 1)).item()                \n",
    "\n",
    "            mean_valid_cls_acc = valid_class_acc/(batch_cnt + 1)\n",
    "            mean_valid_led_acc = valid_lead_acc/(batch_cnt + 1)\n",
    "            val_losses.append(val_loss)\n",
    "            self.checkpoint(valid_lead_acc = mean_valid_cls_acc, \n",
    "                       valid_class_acc = mean_valid_led_acc, \n",
    "                       val_total_loss = val_loss,\n",
    "                       epoch = epoch, \n",
    "                       model = self.model,\n",
    "                       optimizer = self.optimizer,\n",
    "                       check_folder = 'run_efficientB0')\n",
    "\n",
    "            print(f\"Val_loss: {val_loss.item()} - Val class acc: {mean_valid_cls_acc} - Val lead acc: {mean_valid_led_acc}\")\n",
    "        \n",
    "        return val_loss, mean_valid_cls_acc, mean_valid_led_acc\n",
    "\n",
    "    def loss_plot(self, train_losses, val_losses, n_epochs, check_folder):\n",
    "        now = datetime.now().strftime(\"%m-%d-%Y - %H-%M-%S\")\n",
    "        run_dir = os.getcwd() + f\"/{check_folder}\"\n",
    "        if not os.path.exists(run_dir):\n",
    "            os.mkdir(run_dir)\n",
    "        save_dir = run_dir + f\"/{now}\"\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        save_loss_dir = save_dir + \"\\save_losses\"\n",
    "        if not os.path.exists(save_loss_dir):\n",
    "            os.mkdir(save_loss_dir)\n",
    "        save_fig_losses = os.path.join(save_loss_dir, 'plot_losses.png')  \n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(range(1, n_epochs + 1), train_losses, label='Train Loss')\n",
    "        plt.plot(range(1, n_epochs + 1), val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        # plt.xticks()\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig(save_fig_losses)\n",
    "    \n",
    "    def checkpoint(self, valid_lead_acc, valid_class_acc, \n",
    "                   val_total_loss,\n",
    "                   epoch, \n",
    "                   model,\n",
    "                   optimizer,\n",
    "                   check_folder\n",
    "#                    logs\n",
    "                  ):\n",
    "        old_valid_lead_acc = 0\n",
    "        old_valid_class_acc = 0\n",
    "        old_valid_loss = 1e23\n",
    "        if valid_lead_acc >= old_valid_lead_acc and valid_class_acc >= old_valid_class_acc and val_total_loss <= old_valid_loss:\n",
    "            old_valid_lead_acc = valid_lead_acc\n",
    "            old_valid_class_acc = valid_class_acc\n",
    "            old_valid_loss = val_total_loss\n",
    "            save_dict = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': val_total_loss,\n",
    "                'val_lead_acc': valid_lead_acc,\n",
    "                'val_class_acc': valid_class_acc\n",
    "            }\n",
    "            \n",
    "         # Saving best model\n",
    "        now = datetime.now().strftime(\"%m-%d-%Y - %H-%M-%S\")\n",
    "        run_dir = os.getcwd() + f\"/{check_folder}\"\n",
    "        if not os.path.exists(run_dir):\n",
    "            os.mkdir(run_dir)\n",
    "        save_dir = run_dir + f\"/{now}\"\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        save_best_model_dir = save_dir + \"\\save_best_model\"\n",
    "        if not os.path.exists(save_best_model_dir):\n",
    "            os.mkdir(save_best_model_dir)\n",
    "        save_best_model_path = save_best_model_dir + f\"/{save_dict['loss']:>7f}_{save_dict['val_lead_acc']:>7f}_{save_dict['val_class_acc']:>7f}_{now}.pt\"\n",
    "        torch.save(save_dict, save_best_model_path)\n",
    "\n",
    "#         config_path = save_dir + '/config.json'\n",
    "#         with open(config_path, mode='w') as file:\n",
    "#             json.dump(vars(args), file)\n",
    "    \n",
    "    def evaluation(self):\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        y1_eval = [] \n",
    "        y2_eval = []\n",
    "        pred1_eval = []\n",
    "        pred2_eval = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch, (y1, y2, X) in tqdm(enumerate(self.test_data)):\n",
    "                batch_cnt = batch\n",
    "                \n",
    "                y1, y2 = y1.to(self.device), y2.to(self.device)\n",
    "                pred1, pred2 = self.model((X/255).to(self.device))\n",
    "                \n",
    "                y1_eval.append(torch.argmax(y1, dim=1)[0])\n",
    "                y2_eval.append(torch.argmax(y2, dim=1)[0])\n",
    "                pred1_eval.append(torch.argmax(pred1, dim=1)[0])\n",
    "                pred2_eval.append(torch.argmax(pred2, dim=1)[0])\n",
    "                \n",
    "            y1_eval = torch.stack(y1_eval)\n",
    "            y2_eval = torch.stack(y2_eval)\n",
    "            pred1_eval = torch.stack(pred1_eval)\n",
    "            pred2_eval = torch.stack(pred2_eval)\n",
    "                \n",
    "            pred1 = torch.nn.functional.one_hot(pred1_eval, num_classes = 12)\n",
    "            pred2 = torch.nn.functional.one_hot(pred2_eval, num_classes = 9)\n",
    "            y1 = torch.nn.functional.one_hot(y1_eval, num_classes = 12)\n",
    "            y2 = torch.nn.functional.one_hot(y2_eval, num_classes = 9)\n",
    "#             print(y1)\n",
    "#             print(y2)\n",
    "#             print(pred1)\n",
    "#             print(pred2)\n",
    "#             print(y2_eval)\n",
    "#             print(pred2_eval)\n",
    "#             print(torch.argmax(y1, dim=1).cpu().numpy()[0].tolist())\n",
    "#             print(torch.argmax(y1, dim=1).cpu().numpy().tolist())\n",
    "            \n",
    "            # Classification report\n",
    "            for i in range (len(class_la)):\n",
    "                class_la[i] = str(class_la[i])\n",
    "\n",
    "            # Classification report\n",
    "            print(classification_report(\n",
    "                torch.argmax(y1, dim=1).cpu().numpy().tolist(), \n",
    "                torch.argmax(pred1, dim=1).cpu().numpy().tolist(),\n",
    "                target_names=lead))    \n",
    "            \n",
    "\n",
    "            print(classification_report(\n",
    "                torch.argmax(y2, dim=1).cpu().numpy().tolist(), \n",
    "                torch.argmax(pred2, dim=1).cpu().numpy().tolist(), \n",
    "                target_names=class_la)) \n",
    "\n",
    "            # Confusion Matrix\n",
    "            ConfusionMatrixDisplay.from_predictions(torch.argmax(y1, dim=1).cpu().numpy().tolist(), torch.argmax(pred1, dim=1).cpu().numpy().tolist(), cmap = \"PuBuGn\")\n",
    "            plt.show()\n",
    "\n",
    "            ConfusionMatrixDisplay.from_predictions(torch.argmax(y2, dim=1).cpu().numpy().tolist(), torch.argmax(pred2, dim=1).cpu().numpy().tolist(), cmap = \"PuBuGn\")\n",
    "            plt.show()\n",
    "\n",
    "            # ROC Curve\n",
    "            for i in range(12):\n",
    "                r1 = roc_auc_score(y1[:, i].cpu(), pred1[:, i].cpu())\n",
    "                print(\"The ROC AUC score of \"+ lead[i] +\" is: \"+str(r1))\n",
    "\n",
    "            for i in range(9):\n",
    "                r2 = roc_auc_score(y2[:, i].cpu(), pred2[:, i].cpu())\n",
    "                print(\"The ROC AUC score of \"+ class_la[i] +\" is: \"+str(r2))\n",
    "\n",
    "            # Compute ROC curve and ROC area for each lead\n",
    "            lead_fpr = {}\n",
    "            lead_tpr = {}\n",
    "            roc_auc = {}\n",
    "            lead_roc_auc = dict()\n",
    "            for i in range(12):\n",
    "                lead_fpr[i], lead_tpr[i], _ = roc_curve(y1[:, i].cpu(), pred1[:, i].cpu(), drop_intermediate=False)\n",
    "                roc_auc[i] = auc(lead_fpr[i], lead_tpr[i])\n",
    "\n",
    "            plt.plot(lead_fpr[0], lead_tpr[0],'turquoise',label=f'I: ROC curve of {lead[0]} (area = %0.2f)' % roc_auc[0])\n",
    "            plt.plot(lead_fpr[1], lead_tpr[1],'peachpuff',label=f'II: ROC curve of {lead[1]} (area = %0.2f)' % roc_auc[1])\n",
    "            plt.plot(lead_fpr[2], lead_tpr[2],'paleturquoise',label=f'III: ROC curve of {lead[2]} (area = %0.2f)' % roc_auc[2])\n",
    "            plt.plot(lead_fpr[3], lead_tpr[3],'pink',label=f'aVR: ROC curve of {lead[3]} (area = %0.2f)' % roc_auc[3])\n",
    "            plt.plot(lead_fpr[4], lead_tpr[4],'lightcoral',label=f'aVL: ROC curve of {lead[4]} (area = %0.2f)' % roc_auc[4])\n",
    "            plt.plot(lead_fpr[5], lead_tpr[5],'peachpuff',label=f'aVF: ROC curve of {lead[5]} (area = %0.2f)' % roc_auc[5])\n",
    "            plt.plot(lead_fpr[6], lead_tpr[6],'steelblue',label=f'V1: ROC curve of {lead[6]} (area = %0.2f)' % roc_auc[6])\n",
    "            plt.plot(lead_fpr[7], lead_tpr[7],'forestgreen',label=f'V2: ROC curve of {lead[7]} (area = %0.2f)' % roc_auc[7])\n",
    "            plt.plot(lead_fpr[8], lead_tpr[8],'darkslategray',label=f'V3: ROC curve of {lead[8]} (area = %0.2f)' % roc_auc[8])\n",
    "            plt.plot(lead_fpr[9], lead_tpr[9],'orange',label=f'V4: ROC curve of {lead[9]} (area = %0.2f)' % roc_auc[9])\n",
    "            plt.plot(lead_fpr[10], lead_tpr[10],'maroon',label=f'V5: ROC curve of {lead[10]} (area = %0.2f)' % roc_auc[10])\n",
    "            plt.plot(lead_fpr[11], lead_tpr[11],'navy',label=f'V6: ROC curve of {lead[11]} (area = %0.2f)' % roc_auc[11])\n",
    "\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.xlim([-0.1, 1.1])\n",
    "            plt.ylim([-0.1, 1.1])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('Receiver operating characteristic of class')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.show()\n",
    "\n",
    "            # Compute ROC curve and ROC area for each class\n",
    "            class_fpr = {}\n",
    "            class_tpr = {}\n",
    "            class_roc_auc = dict()\n",
    "            for i in range(9):\n",
    "                class_fpr[i], class_tpr[i], _ = roc_curve(y2[:, i].cpu(), pred2[:, i].cpu(), drop_intermediate=False)\n",
    "                roc_auc[i] = auc(class_fpr[i], class_tpr[i])\n",
    "\n",
    "            plt.plot(class_fpr[0], class_tpr[0],'turquoise',label='1: ROC curve of class 1 (area = %0.2f)' % roc_auc[0])\n",
    "            plt.plot(class_fpr[1], class_tpr[1],'peachpuff',label='2: ROC curve of class 2 (area = %0.2f)' % roc_auc[1])\n",
    "            plt.plot(class_fpr[2], class_tpr[2],'paleturquoise',label='3: ROC curve of class 3 (area = %0.2f)' % roc_auc[2])\n",
    "            plt.plot(class_fpr[3], class_tpr[3],'pink',label='4: ROC curve of class 4 (area = %0.2f)' % roc_auc[3])\n",
    "            plt.plot(class_fpr[4], class_tpr[4],'lightcoral',label='5: ROC curve of class 5 (area = %0.2f)' % roc_auc[4])\n",
    "            plt.plot(class_fpr[5], class_tpr[5],'peachpuff',label='6: ROC curve of class 6 (area = %0.2f)' % roc_auc[5])\n",
    "            plt.plot(class_fpr[6], class_tpr[6],'steelblue',label='7: ROC curve of class 7 (area = %0.2f)' % roc_auc[6])\n",
    "            plt.plot(class_fpr[7], class_tpr[7],'forestgreen',label='8: ROC curve of class 8 (area = %0.2f)' % roc_auc[6])\n",
    "            plt.plot(class_fpr[8], class_tpr[8],'darkslategray',label='9: ROC curve of class 9 (area = %0.2f)' % roc_auc[6])\n",
    "\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.xlim([-0.1, 1.1])\n",
    "            plt.ylim([-0.1, 1.1])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('Receiver operating characteristic of lead')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.show()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafd1b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240it [01:12,  3.44it/s]"
     ]
    }
   ],
   "source": [
    "# Metadata\n",
    "\n",
    "# Training Setup\n",
    "device = \"cuda\"\n",
    "learning_rate = 0.0001\n",
    "optimizer = \"Adam\"\n",
    "loss = \"CCE\"\n",
    "# batchsize = 128\n",
    "epochs = 100\n",
    "# ratio = (0.8, 0.1, 0.1)\n",
    "\n",
    "# Data\n",
    "\n",
    "monitor = Training(device = device,\n",
    "                   learning_rate = learning_rate,\n",
    "                   optimizer = optimizer,\n",
    "                   loss = loss,\n",
    "                   model = model,\n",
    "#                    batchsize = batchsize,\n",
    "                   epochs = epochs)\n",
    "#                    label_df = label_df, \n",
    "#                    root_dir = img_data_dir, \n",
    "#                    ratio = ratio)\n",
    "monitor.__update__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7d181ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unitest\n",
    "\n",
    "# monitor.get_sample_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99c51f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df57c6c",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b71e558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor.evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cbcfa8",
   "metadata": {},
   "source": [
    "# Clear Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05eaee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor.clear_buffer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
