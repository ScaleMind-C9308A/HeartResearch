{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6169743",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "038e91c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7302e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import os, sys, shutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82a9d0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\GitCloneProject\\HeartResearch\\Experiment\\Approach\\EfficientB0\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f58c040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\GitCloneProject\\HeartResearch\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9ae6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425820\n"
     ]
    }
   ],
   "source": [
    "# D:\\GitCloneProject\\HeartResearch\\Data set\\v4_data\\med_scaleogram_h256_w512_seglen1600_scl500\n",
    "main_data_dir = os.getcwd() + \"\\\\Data set\"\n",
    "\n",
    "label_csv_path = main_data_dir + \"\\\\Label.csv\"\n",
    "\n",
    "img_data_dir = main_data_dir + \"\\\\v4_data\\\\med_scaleogram_h256_w512_seglen1600_scl500\"\n",
    "print(len(os.listdir(img_data_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "237a8291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recording</th>\n",
       "      <th>First_label</th>\n",
       "      <th>Second_label</th>\n",
       "      <th>Third_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0001</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0002</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0003</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0004</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0005</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Recording  First_label  Second_label  Third_label\n",
       "0     A0001            5           NaN          NaN\n",
       "1     A0002            1           NaN          NaN\n",
       "2     A0003            2           NaN          NaN\n",
       "3     A0004            2           NaN          NaN\n",
       "4     A0005            7           NaN          NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df = pd.read_csv(label_csv_path)\n",
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "825bb663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    1695\n",
       "2    1098\n",
       "1     918\n",
       "8     826\n",
       "3     704\n",
       "7     653\n",
       "6     574\n",
       "4     207\n",
       "9     202\n",
       "Name: First_label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df[\"First_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189b94c0",
   "metadata": {},
   "source": [
    " # Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39c47eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.nn.functional import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c33309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeartDisease(Dataset):\n",
    "    def __init__(self, label_df: pd.DataFrame = label_df, \n",
    "                 root_dir: str = img_data_dir, \n",
    "                 ratio: tuple = (0.8, 0.1, 0.1),\n",
    "                 subset = \"training\", # validating, testing\n",
    "                 resize: tuple = None, #  (256, 512)\n",
    "                 seed: int = 777\n",
    "                ): \n",
    "        \n",
    "        self.label_df = label_df\n",
    "        self.label_col = torch.tensor(self.label_df[\"First_label\"].values - np.ones_like(self.label_df[\"First_label\"].values.shape))\n",
    "        self.onehot_label = one_hot(self.label_col)\n",
    "        \n",
    "        self.label_dict = {\n",
    "            name : vector for name, vector in zip(self.label_df[\"Recording\"], self.onehot_label)\n",
    "        }\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.ratio = ratio\n",
    "        self.subset = subset\n",
    "        self.resize = resize\n",
    "        self.fullpaths = glob(self.root_dir + \"\\\\*\")    \n",
    "        random.Random(seed).shuffle(self.fullpaths)\n",
    "        self.start = int(len(self.fullpaths) * self.ratio[0])\n",
    "        self.mid = int(len(self.fullpaths) * self.ratio[1])\n",
    "        self.stop = int(len(self.fullpaths) * self.ratio[2])\n",
    "        if self.subset == \"training\":\n",
    "            self.imgpaths = self.fullpaths[ : self.start]\n",
    "        elif self.subset == \"validating\":\n",
    "            self.imgpaths = self.fullpaths[self.start : self.start + self.mid]\n",
    "        elif self.subset == \"testing\":\n",
    "            self.imgpaths = self.fullpaths[self.start + self.mid : self.start + self.mid + self.stop]\n",
    "        else:\n",
    "            raise Exception(f'Invalid subset. Subset Arg should be training, validating, and testing but \"{self.subset}\" found')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgpaths)\n",
    "\n",
    "    def __getitem__(self, idx:list = None):\n",
    "        if idx == None:\n",
    "            raise Exception('idx arg cannot be None')\n",
    "            \n",
    "        imgs = []\n",
    "        labels= []\n",
    "        \n",
    "        for i in idx:\n",
    "            temp_path = self.imgpaths[i]\n",
    "            \n",
    "            filename = temp_path.split(\"\\\\\")[-1].split(\"_\")[0]\n",
    "            labels.append(self.label_dict[filename])\n",
    "            \n",
    "            temp_img = cv.imread(temp_path)\n",
    "            temp_img = torch.tensor(temp_img).permute(-1, 0, 1)\n",
    "            imgs.append(temp_img)\n",
    "        \n",
    "        batch_imgs = torch.stack(imgs)\n",
    "        batch_labels = torch.stack(labels)\n",
    "\n",
    "        return batch_imgs, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad0e825",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "651dfeb8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from torch import nn\n",
    "\n",
    "class HeartModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ori_model = efficientnet_b0(weights = EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        self.ori_model.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1280, 9),\n",
    "            nn.Softmax(dim = 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        x = self.ori_model(x)\n",
    "        return x\n",
    "\n",
    "model = HeartModel().to(\"cuda\")\n",
    "x = torch.randn((1, 3, 256, 512)).to(\"cuda\")\n",
    "out = model(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3843ece2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeartModel(\n",
       "  (ori_model): EfficientNet(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (8): Conv2dNormActivation(\n",
       "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Sequential(\n",
       "      (0): Dropout(p=0.2, inplace=False)\n",
       "      (1): Linear(in_features=1280, out_features=9, bias=True)\n",
       "      (2): Softmax(dim=1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe4995",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8989c016",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: unmatched '[' (4287155738.py, line 169)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [12], line 169\u001b[1;36m\u001b[0m\n\u001b[1;33m    raise Exception(f\"early_stop arg should have the value val_acc, val_loss, train_acc, train_loss but found {early_stop[\"object\"]} instead\")\u001b[0m\n\u001b[1;37m                                                                                                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m f-string: unmatched '['\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "opt_mapping = {\n",
    "    \"Adam\" : torch.optim.Adam\n",
    "}\n",
    "\n",
    "loss_mapping = {\n",
    "    \"CCE\" : nn.CrossEntropyLoss()\n",
    "}\n",
    "\n",
    "class Training:\n",
    "    def __init__(self, \n",
    "                 device: str = \"cuda\",\n",
    "                 learning_rate:float = 0.0001,\n",
    "                 optimizer:str = \"Adam\",\n",
    "                 loss:str = \"CCE\",\n",
    "                 model = model,\n",
    "                 batchsize:int = 32,\n",
    "                 epochs:int = 1,\n",
    "                 label_df: pd.DataFrame = label_df, \n",
    "                 root_dir: str = img_data_dir, \n",
    "                 ratio: tuple = (0.8, 0.1, 0.1),\n",
    "                 mode:str = \"development\",\n",
    "                 resize: tuple = None, #  (256, 512)\n",
    "                 seed: int = 777,\n",
    "                 early_stop: dict = {\n",
    "                     \"patience\" : 10,\n",
    "                     \"object\" : \"train_acc\", # train_loss, val_loss, val_acc\n",
    "                 },\n",
    "                 \n",
    "                 model_check_point: bool = True\n",
    "                ):\n",
    "        \n",
    "        # Setup\n",
    "        self.device = device\n",
    "        self.current_time = datetime.now().strftime(\"%m-%d-%Y - %H-%M-%S\")\n",
    "        self.model = model\n",
    "        self.model.to(self.device)\n",
    "        self.lr = learning_rate\n",
    "        self.optimizer = opt_mapping[optimizer](self.model.parameters(), lr=self.lr)\n",
    "        self.loss_fn = loss_mapping[loss]\n",
    "        self.bs = batchsize\n",
    "        self.ep = epochs\n",
    "        self.default_ratio = (0.001, 0.001, 0.001)\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=9)\n",
    "        self.target_names = [str(i) for i in range(9)]\n",
    "        \n",
    "        if early_stop:\n",
    "            self.early_stop = early_stop\n",
    "        \n",
    "        # Data\n",
    "        self.label_df = label_df\n",
    "        self.root_dir = root_dir\n",
    "        if mode == \"experiment\":\n",
    "            self.ratio = ratio\n",
    "        elif mode == \"development\":\n",
    "            self.ratio = self.default_ratio\n",
    "        self.resize = resize\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.train_data = HeartDisease(label_df = self.label_df, \n",
    "                 root_dir = self.root_dir, \n",
    "                 ratio = self.ratio,\n",
    "                 subset = \"training\",\n",
    "                 resize = self.resize,\n",
    "                 seed = self.seed)\n",
    "        \n",
    "        self.valid_data = HeartDisease(label_df = self.label_df, \n",
    "                 root_dir = self.root_dir, \n",
    "                 ratio = self.ratio,\n",
    "                 subset = \"validating\",\n",
    "                 resize = self.resize,\n",
    "                 seed = self.seed)\n",
    "        \n",
    "        self.test_data = HeartDisease(label_df = self.label_df, \n",
    "                 root_dir = self.root_dir, \n",
    "                 ratio = self.ratio,\n",
    "                 subset = \"testing\",\n",
    "                 resize = self.resize,\n",
    "                 seed = self.seed)\n",
    "        \n",
    "        # Logging\n",
    "        self.log_dict = {\n",
    "            \"train_loss\" : [],\n",
    "            \"train_acc\" : [],\n",
    "            \"val_loss\" : [],\n",
    "            \"val_acc\" : []\n",
    "        }\n",
    "    \n",
    "    def get_sample_count(self):\n",
    "        print(f\"Training: {len(self.train_data)}\")\n",
    "        print(f\"Validating: {len(self.valid_data)}\")\n",
    "        print(f\"Testing: {len(self.test_data)}\")\n",
    "    \n",
    "    def logging(self, train_loss:float , \n",
    "                train_acc: float, valid_loss:float, valid_acc:float):\n",
    "        self.log_dict[\"train_loss\"].append(train_loss)\n",
    "        self.log_dict[\"train_acc\"].append(train_acc)\n",
    "        self.log_dict[\"valid_loss\"].append(valid_loss)\n",
    "        self.log_dict[\"valid_acc\"].append(valid_acc)\n",
    "    \n",
    "    def export_log(self, rdir:str = None, extenstion:str = \"parquet\"):\n",
    "        if not rdir:\n",
    "            file_path = os.getcwd() + f\"/result/\"\n",
    "        pass\n",
    "    \n",
    "    def export_model(self): # filename = val_loss + val_acc\n",
    "        pass\n",
    "    \n",
    "    def clear_buffer(self):\n",
    "        del self.train_data\n",
    "        del self.test_data\n",
    "        del self.valid_data\n",
    "        del self.model\n",
    "    \n",
    "    def update(self):\n",
    "        if early_stop:\n",
    "            self.checkpoint = 0\n",
    "            if \"acc\" in early_stop[\"object\"]:\n",
    "                self.last_value = 0\n",
    "            elif \"loss\" in early_stop[\"object\"]:\n",
    "                self.last_value = 100\n",
    "        \n",
    "        self.model.train()\n",
    "        for e in range(self.ep):\n",
    "            for batch in trange(0, len(self.train_data), self.bs):\n",
    "                if len(self.train_data) - batch > self.bs:\n",
    "                    batch_indices = [x for x in range(batch, batch + self.bs)]\n",
    "                else:\n",
    "                    batch_indices = [x for x in range(batch, len(self.train_data))]\n",
    "                X, y = self.train_data[batch_indices]\n",
    "\n",
    "                pred = self.model((X/255).to(self.device))\n",
    "                loss = self.loss_fn(pred, y.to(self.device, dtype = torch.float))\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "            train_acc = self.accuracy(torch.argmax(pred, dim = 1), torch.argmax(y, dim = 1))\n",
    "\n",
    "            val_loss, val_acc = self.validation()\n",
    "\n",
    "            # Show train_loss, train_acc, val_loss, val_acc\n",
    "            print(f\"Epoch: {e} - Train Loss: {loss.item()} - Train Acc: {train_acc} - Val Loss: {val_loss.item()} - Val Acc: {val_acc.item()}\")\n",
    "            \n",
    "            if early_stop:\n",
    "                if early_stop[\"object\"] == \"train_loss\":\n",
    "                    if loss.item() > self.last_value:\n",
    "                        self.check_point += 1\n",
    "                    else:\n",
    "                        self.last_value = loss.item()\n",
    "                elif early_stop[\"object\"] == \"train_acc\":\n",
    "                    if train_acc < self.last_value:\n",
    "                        self.check_point += 1\n",
    "                    else:\n",
    "                        self.last_value = train_acc\n",
    "                elif early_stop[\"object\"] == \"val_loss\":\n",
    "                    if val_loss.item() > self.last_value:\n",
    "                        self.check_point += 1\n",
    "                    else:\n",
    "                        self.last_value = val_loss.item()\n",
    "                elif early_stop[\"object\"] == \"val_acc\":\n",
    "                    if val_acc < self.last_value:\n",
    "                        self.check_point += 1\n",
    "                    else:\n",
    "                        self.last_value = val_acc\n",
    "                else:\n",
    "                    raise Exception(f\"early_stop arg should have the value val_acc, val_loss, train_acc, train_loss but found {early_stop[\"object\"]} instead\")\n",
    "            \n",
    "            elif model_check_point:\n",
    "                \n",
    "                \n",
    "\n",
    "        if self.check_point >= self.early_stop[\"patience\"]:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    def validation(self):\n",
    "        loss = 0\n",
    "        target = []\n",
    "        preds = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in trange(0, len(self.valid_data), self.bs):\n",
    "                if len(self.valid_data) - batch > self.bs:\n",
    "                    batch_indices = [x for x in range(batch, batch + self.bs)]\n",
    "                else:\n",
    "                    batch_indices = [x for x in range(batch, len(self.valid_data))]\n",
    "                X, y = self.valid_data[batch_indices]\n",
    "                pred = self.model((X/255).to(self.device))\n",
    "\n",
    "                loss_out = self.loss_fn(pred, y.to(self.device, dtype = torch.float))\n",
    "                \n",
    "                loss += loss_out\n",
    "                \n",
    "                target.append(y[0])\n",
    "                preds.append(pred[0])\n",
    "                \n",
    "            target = torch.stack(target)\n",
    "            preds = torch.stack(preds)\n",
    "            loss = loss/len(self.valid_data)\n",
    "            \n",
    "            return loss, self.accuracy(torch.argmax(preds, dim = 1), torch.argmax(target, dim = 1))\n",
    "        \n",
    "    def evaluation(self):\n",
    "        target = []\n",
    "        preds = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in range(len(self.test_data)):\n",
    "                X, y = self.test_data[[batch]]\n",
    "                pred = self.model((X/255).to(self.device))\n",
    "                \n",
    "                target.append(y[0])\n",
    "                preds.append(pred[0])\n",
    "                \n",
    "            target = torch.stack(target)\n",
    "            preds = torch.stack(preds)\n",
    "            \n",
    "            # Classification report\n",
    "            print(classification_report(\n",
    "                torch.argmax(target, dim=1).numpy().tolist(), \n",
    "                torch.argmax(preds, dim=1).numpy().tolist(), \n",
    "                target_names=self.target_names))            \n",
    "            \n",
    "            # Confusion Matrix\n",
    "            \n",
    "            # ROC Curve\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafd1b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata\n",
    "\n",
    "# Training Setup\n",
    "device = \"cpu\"\n",
    "learning_rate = 0.0001\n",
    "optimizer = \"Adam\"\n",
    "loss = \"CCE\"\n",
    "batchsize = 128\n",
    "epochs = 50\n",
    "ratio = (0.8, 0.1, 0.1)\n",
    "mode = \"development\"\n",
    "seed = 123\n",
    "\n",
    "# Data\n",
    "\n",
    "monitor = Training(device = device,\n",
    "                   learning_rate = learning_rate,\n",
    "                   optimizer = optimizer,\n",
    "                   loss = loss,\n",
    "                   model = model,\n",
    "                   batchsize = batchsize,\n",
    "                   epochs = epochs,\n",
    "                   label_df = label_df, \n",
    "                   root_dir = img_data_dir, \n",
    "                   ratio = ratio,\n",
    "                   mode = mode,\n",
    "                   seed = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d181ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unitest\n",
    "\n",
    "monitor.get_sample_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c51f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df57c6c",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b71e558",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor.evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cbcfa8",
   "metadata": {},
   "source": [
    "# Clear Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eaee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor.clear_buffer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
