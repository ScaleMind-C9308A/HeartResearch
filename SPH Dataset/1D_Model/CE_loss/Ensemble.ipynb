{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6faaf97a-8eb7-40e4-a9ab-f966ebd35d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32994f66-df1a-4c3f-bc80-f7a2b86f0b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['metadata.csv', 'data_df.csv', 'data_df_no1.csv', 'records']\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/media/mountHDD3/data_storage/biomedical_data/ecg_data/SPH\"\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e78b18f3-9b69-41d2-8e9c-3bf2b48da93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20835, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df = pd.read_csv(data_dir + \"/data_df_no1.csv\")\n",
    "main_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4928fd4a-15c1-42f4-9a7e-b60ad642dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_fns = main_df[\"File name\"].values.tolist()\n",
    "single_mat_paths = [data_dir + f\"/records/{x}.h5\" for x in single_fns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "415e67b0-25d4-4bee-85f7-a79a7529fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = [0.8, 0.1]\n",
    "\n",
    "train_index = int(len(single_mat_paths)*ratio[0])\n",
    "valid_index = int(len(single_mat_paths)*(ratio[0]+ratio[1]))\n",
    "\n",
    "train_mat_paths = single_mat_paths[:train_index]\n",
    "valid_mat_paths = single_mat_paths[valid_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7da0bbe-e83b-4ddf-9205-70c6a0c3c325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "class HeartData(Dataset):\n",
    "    def __init__(self, data_paths):\n",
    "        self.data_paths = data_paths\n",
    "        random.shuffle(self.data_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_path = self.data_paths[idx]\n",
    "        a = h5py.File(data_path, 'r')\n",
    "        data_h5 = a['ecg']\n",
    "        data = np.array(data_h5)\n",
    "        clip_data = data[:, 500:5000]\n",
    "\n",
    "        filename = data_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        label = main_df[main_df[\"File name\"] == filename][\"New Label\"].values.item()\n",
    "\n",
    "        torch_data = torch.from_numpy(clip_data)\n",
    "\n",
    "        return torch_data.float(), label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5def9b1-1b2f-4403-a1b8-561c07c88685",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = HeartData(train_mat_paths)\n",
    "valid_ds = HeartData(valid_mat_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87aee458-9186-4726-a2a0-f21a4e56a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size = 64, shuffle = True, pin_memory = True, num_workers = 48)\n",
    "valid_dl = DataLoader(valid_ds, batch_size = 64, shuffle = True, pin_memory = True, num_workers = 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7013d523-dceb-4c08-a590-55ac5ba9b512",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoost_Data(Dataset):\n",
    "    def __init__(self, data_paths):\n",
    "        self.data_paths = data_paths\n",
    "        random.shuffle(self.data_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_path = self.data_paths[idx]\n",
    "        a = h5py.File(data_path, 'r')\n",
    "        data_h5 = a['ecg']\n",
    "        data = np.array(data_h5)\n",
    "        clip_data = data[:, 500:5000]\n",
    "\n",
    "        data_fea = []\n",
    "        \n",
    "        for i in range (12):\n",
    "            list_features = []\n",
    "            data = clip_data[i]\n",
    "            list_features.append(np.mean(data))\n",
    "            list_features.append(np.median(data))\n",
    "            list_features.append(np.std(data))\n",
    "            list_features.append(np.max(data)-np.min(data))\n",
    "            q3, q1 = np.percentile(data, [75 ,25])\n",
    "            list_features.append(q3 - q1)\n",
    "            sk = scipy.stats.skew(data) \n",
    "            list_features.append(sk)\n",
    "            kur = scipy.stats.kurtosis(data)\n",
    "            list_features.append(kur)\n",
    "            data_fea.append(list_features)\n",
    "\n",
    "        data_fea = torch.tensor(data_fea)\n",
    "        data_all = torch.cat((data_fea[0], data_fea[1], data_fea[2], data_fea[3], data_fea[3], data_fea[5], \n",
    "                              data_fea[6], data_fea[7], data_fea[8], data_fea[9], data_fea[10], data_fea[11]))\n",
    "\n",
    "        filename = data_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        label = main_df[main_df[\"File name\"] == filename][\"New Label\"].values.item()\n",
    "\n",
    "        # torch_data = torch.from_numpy(data_all)\n",
    "\n",
    "        return data_all.float(), label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bff80304-8002-4d71-9a68-c3af10d04bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_CelebA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_CelebA, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            # 12*4500\n",
    "            nn.Conv1d(12, 32, kernel_size= 3, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size= 3, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(64, 128, kernel_size= 3, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(128, 256, kernel_size= 3, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(256, 512, kernel_size= 3, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Dropout(p=0.2, inplace=False),\n",
    "            nn.Linear(72192, out_features=31, bias=True)\n",
    "        ) \n",
    "\n",
    "        self.z_mean = nn.Linear(72192, 128)\n",
    "        self.z_log_var = nn.Linear(72192, 128)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(128, 72192),\n",
    "            nn.Unflatten(1, (512, 141)),\n",
    "            nn.ConvTranspose1d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=3, stride=2, padding=2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(),            \n",
    "            nn.ConvTranspose1d(32, 32, kernel_size=3, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(32, 12, kernel_size= 3, padding= 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc = self.encoder(x)\n",
    "        logit = self.cls(enc)\n",
    "        mu = self.z_mean(enc)\n",
    "        lv = self.z_log_var(enc)\n",
    "        lat = self.reparam(mu, lv)\n",
    "        dec = self.decoder(lat)\n",
    "        \n",
    "        return dec, mu, lv, logit\n",
    "\n",
    "    def reparam(self, mu, lv):\n",
    "        std = torch.exp(0.5 * lv)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def get_latent(self, x):\n",
    "        enc = self.encoder(x)\n",
    "        mu = self.z_mean(enc)\n",
    "        lv = self.z_log_var(enc)\n",
    "        lat = self.reparam(mu, lv)\n",
    "\n",
    "        return lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd588823-3881-4d2c-87f8-1f77e28534f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.Sequential(\n",
    "#     # 12*4500\n",
    "#     nn.Conv1d(12, 32, kernel_size= 3, stride=2, padding=1),\n",
    "#     nn.BatchNorm1d(32),\n",
    "#     nn.LeakyReLU(),\n",
    "#     nn.Conv1d(32, 64, kernel_size= 3, stride=2, padding=1),\n",
    "#     nn.BatchNorm1d(64),\n",
    "#     nn.LeakyReLU(),\n",
    "#     nn.Conv1d(64, 128, kernel_size= 3, stride=2, padding=1),\n",
    "#     nn.BatchNorm1d(128),\n",
    "#     nn.LeakyReLU(),\n",
    "#     nn.Conv1d(128, 256, kernel_size= 3, stride=2, padding=1),\n",
    "#     nn.BatchNorm1d(256),\n",
    "#     nn.LeakyReLU(),\n",
    "#     nn.Conv1d(256, 512, kernel_size= 3, stride=2, padding=1),\n",
    "#     nn.BatchNorm1d(512),\n",
    "#     nn.LeakyReLU(),\n",
    "#     nn.Flatten()\n",
    "# )\n",
    "# signal = torch.rand(1, 12, 4500)\n",
    "# a = model(signal)\n",
    "# print(a.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dd9c56d-7095-4f67-8e0f-9027c1d4fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, channel_num):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(channel_num, channel_num, 3, padding=1),\n",
    "\t\t\tnn.BatchNorm1d(channel_num),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(channel_num, channel_num, 3, padding=1),\n",
    "\t\t\tnn.BatchNorm1d(channel_num),\n",
    "\t\t)\n",
    "        self.relu = nn.ReLU()\n",
    "        torch.nn.init.kaiming_normal_(self.conv_block1[0].weight)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_block2[0].weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = x + residual\n",
    "        out = self.relu(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3107ac8-bf08-41cd-8140-90193cf8fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, in_channels = 12, type = 18, num_classes = 31):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.struc_dict = {\n",
    "            18: {\n",
    "                \"num_channels\" : [64, 128, 256, 512],\n",
    "                \"counts\" : [2, 2, 2, 2]\n",
    "            }\n",
    "        }\n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels, out_channels=64, kernel_size=7, stride=2)\n",
    "        torch.nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        self.max1 = nn.MaxPool1d(kernel_size=3, stride=2)\n",
    "        self.main = nn.Sequential()\n",
    "        for idx, struc in enumerate(\n",
    "            zip(\n",
    "                self.struc_dict[type][\"num_channels\"], \n",
    "                self.struc_dict[type][\"counts\"]\n",
    "            )\n",
    "        ):\n",
    "            num_channel, cnt = struc\n",
    "            for i in range(cnt):\n",
    "                self.main.add_module(f\"conv{idx+1}_{i}\", BasicBlock(num_channel))\n",
    "            if idx < len(self.struc_dict[type][\"num_channels\"]) - 1:\n",
    "                self.main.add_module(f\"ext_{idx}\", nn.Conv1d(num_channel, self.struc_dict[type][\"num_channels\"][idx+1], 3, 1))\n",
    "                self.main.add_module(f\"extbn_{idx}\", nn.BatchNorm1d(self.struc_dict[type][\"num_channels\"][idx+1]))\n",
    "                                     \n",
    "        self.avg = torch.nn.AdaptiveAvgPool1d((1))\n",
    "        self.lin = nn.Linear(self.struc_dict[type][\"num_channels\"][-1], num_classes)\n",
    "        torch.nn.init.kaiming_normal_(self.lin.weight)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.max1(x)\n",
    "        x = self.main(x)\n",
    "        x = self.avg(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35414e01-618c-436b-947a-8f2bfa75d675",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ResNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m best_ep \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m resnet \u001b[38;5;241m=\u001b[39m \u001b[43mResNet\u001b[49m()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m CVAE \u001b[38;5;241m=\u001b[39m CNN_CelebA()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m optimizer_rn \u001b[38;5;241m=\u001b[39m Adam(resnet\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ResNet' is not defined"
     ]
    }
   ],
   "source": [
    "epoch = 150\n",
    "lr = 0.0005\n",
    "best_acc = 0\n",
    "best_ep = 0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\", index = 1)\n",
    "\n",
    "resnet = ResNet().to(device)\n",
    "CVAE = CNN_CelebA().to(device)\n",
    "\n",
    "optimizer_rn = Adam(resnet.parameters(), lr=lr)\n",
    "scheduler_rn = CosineAnnealingLR(optimizer=optimizer_rn, T_max=epoch)\n",
    "\n",
    "optimizer_ae = Adam(CVAE.parameters(), lr=lr)\n",
    "scheduler_ae = CosineAnnealingLR(optimizer=optimizer_ae, T_max=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9215ed9a-5f16-4e4d-9ca3-6b0ac4520ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_loss = nn.MSELoss(reduction='none')\n",
    "\n",
    "def loss_fn_sig(recon_x, x):\n",
    "    return torch.mean(torch.sum(recon_loss(recon_x, x), dim=(1,2)))\n",
    "\n",
    "def gaussian_kls(mu, logvar):\n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "    return torch.mean(kld_loss)\n",
    "\n",
    "loss_fn_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "for e in range(epoch):\n",
    "    CVAE.train()\n",
    "    print(f\"Epoch: {e}\")\n",
    "    y_true_list = [] \n",
    "    pred_list = []\n",
    "    batch_cnt = 0\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for batch, (train_sig, train_label) in tqdm(enumerate(train_dl)):\n",
    "        batch_cnt = batch\n",
    "        train_sig = train_sig.to(device)\n",
    "        train_label = train_label.to(device)\n",
    "        \n",
    "        res_sig, train_mu, train_lv, pred_cls = CVAE(train_sig)\n",
    "        loss_cls = loss_fn_cls(pred_cls, train_label)\n",
    "        loss_sig = loss_fn_sig(res_sig, train_sig)\n",
    "        loss_kl = gaussian_kls(train_mu, train_lv)\n",
    "        loss_tot = loss_cls + loss_sig + loss_kl\n",
    "        \n",
    "        optimizer_ae.zero_grad()\n",
    "        loss_tot.backward()\n",
    "        optimizer_ae.step()\n",
    "        \n",
    "        scheduler_ae.step()\n",
    "        \n",
    "        total_loss += loss_tot.item()\n",
    "        correct += (pred_cls.argmax(1) == train_label).type(torch.float).sum().item()\n",
    "    \n",
    "    total_loss /= batch_cnt\n",
    "    correct /= len(train_dl.dataset)\n",
    "    \n",
    "    print(f\"train loss: {total_loss} - train acc: {100*correct}\")\n",
    "    \n",
    "    batch_cnt = 0\n",
    "    val_total_loss = 0\n",
    "    val_correct = 0\n",
    "    CVAE.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch, (valid_sig, valid_label) in tqdm(enumerate(valid_dl)):\n",
    "            batch_cnt = batch\n",
    "            valid_sig = valid_sig.to(device)\n",
    "            valid_label = valid_label.to(device)\n",
    "            \n",
    "            res_sig, valid_mu, valid_lv, pred_cls = CVAE(valid_sig)\n",
    "            loss_cls = loss_fn_cls(pred_cls, valid_label)\n",
    "            loss_sig = loss_fn_sig(res_sig, valid_sig)\n",
    "            loss_kl = gaussian_kls(valid_mu, valid_lv)\n",
    "            loss_tot = loss_cls + loss_sig + loss_kl\n",
    "            # print(loss_cls)\n",
    "            # print(loss_sig)\n",
    "            # print(loss_kl)\n",
    "            \n",
    "            pred_pos = pred_cls.argmax(1)\n",
    "            y_true_list.append(valid_label)\n",
    "            pred_list.append(pred_pos)\n",
    "\n",
    "            val_total_loss += loss_tot.item()\n",
    "            val_correct += (pred_cls.argmax(1) == valid_label).type(torch.float).sum().item()\n",
    "    \n",
    "        val_total_loss /= batch_cnt\n",
    "        val_correct /= len(valid_dl.dataset)\n",
    "        if val_correct > best_acc:\n",
    "            best_acc = val_correct\n",
    "            best_ep = e\n",
    "        \n",
    "        print(f\"valid loss: {val_total_loss} - valid acc: {100*val_correct}\")\n",
    "        \n",
    "y_true = torch.cat(y_true_list).cpu().numpy()\n",
    "pred = torch.cat(pred_list).cpu().numpy()\n",
    "\n",
    "reports = classification_report(y_true, pred, output_dict=True)\n",
    "\n",
    "print(reports)\n",
    "print(f\"Best acuracy: {best_acc} at epoch {best_ep}\")\n",
    "\n",
    "# pred_list_test = []\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     va_total_loss = {\n",
    "#         \"rec_loss\" : 0,\n",
    "#         \"kl_loss\" : 0\n",
    "#     }\n",
    "#     for test_sig, test_label in tqdm(test_dl):\n",
    "#         valid_img = valid_img.to(device)\n",
    "\n",
    "#         res_sig, valid_mu, valid_lv, pred_cls = CVAE(test_sig)\n",
    "#         loss_cls = loss_fn_cls(pred_cls, test_label)\n",
    "#         loss_sig = loss_fn_sig(res_sig, test_sig)\n",
    "#         loss_kl = gaussian_kls(valid_mu, valid_lv)\n",
    "#         loss_tot = loss_cls + loss_sig + loss_kl\n",
    "\n",
    "#         pred_pos = pred_cls.argmax(1)\n",
    "#         pred_list_test.append(pred_pos)\n",
    "\n",
    "# print(pred_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "440ac3a8-86fd-41ef-abc0-e0b01df92c40",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mepoch\u001b[49m):\n\u001b[1;32m      4\u001b[0m     resnet\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch' is not defined"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for e in range(epoch):\n",
    "    resnet.train()\n",
    "    print(f\"Epoch: {e}\")\n",
    "    y_true_list = [] \n",
    "    pred_list = []\n",
    "    batch_cnt = 0\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for batch, (train_sig, train_label) in tqdm(enumerate(train_dl)):\n",
    "        batch_cnt = batch\n",
    "        train_sig = train_sig.to(device)\n",
    "        train_label = train_label.to(device)\n",
    "        \n",
    "        pred = resnet(train_sig)\n",
    "        loss = loss_fn(pred, train_label)\n",
    "        \n",
    "        optimizer_rn.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_rn.step()\n",
    "        \n",
    "        scheduler_rn.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == train_label).type(torch.float).sum().item()\n",
    "    \n",
    "    total_loss /= batch_cnt\n",
    "    correct /= len(train_dl.dataset)\n",
    "    \n",
    "    print(f\"train loss: {total_loss} - train acc: {100*correct}\")\n",
    "    \n",
    "    batch_cnt = 0\n",
    "    val_total_loss = 0\n",
    "    val_correct = 0\n",
    "    resnet.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch, (valid_sig, valid_label) in tqdm(enumerate(valid_dl)):\n",
    "            batch_cnt = batch\n",
    "            valid_sig = valid_sig.to(device)\n",
    "            valid_label = valid_label.to(device)\n",
    "            \n",
    "            pred = resnet(valid_sig)\n",
    "            \n",
    "            pred_pos = pred.argmax(1)\n",
    "            y_true_list.append(valid_label)\n",
    "            pred_list.append(pred_pos)\n",
    "            \n",
    "            loss = loss_fn(pred, valid_label)\n",
    "            \n",
    "            val_total_loss += loss.item()\n",
    "            val_correct += (pred.argmax(1) == valid_label).type(torch.float).sum().item()\n",
    "    \n",
    "        val_total_loss /= batch_cnt\n",
    "        val_correct /= len(valid_dl.dataset)\n",
    "        if val_correct > best_acc:\n",
    "            best_acc = val_correct\n",
    "            best_ep = e\n",
    "        \n",
    "        print(f\"valid loss: {val_total_loss} - valid acc: {100*val_correct}\")\n",
    "        \n",
    "y_true = torch.cat(y_true_list).cpu().numpy()\n",
    "pred = torch.cat(pred_list).cpu().numpy()\n",
    "\n",
    "# reports = classification_report(y_true, pred, output_dict=True) \n",
    "\n",
    "# print(reports)\n",
    "print(f\"Best acuracy: {best_acc} at epoch {best_ep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10712be0-ef79-4ab2-9cdc-79cfc529ea0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mountHDD2/thao/git/.env/lib/python3.10/site-packages/scipy/stats/_stats_py.py:1074: RuntimeWarning: overflow encountered in divide\n",
      "  rel_diff = np.max(np.abs(a_zero_mean), axis=axis,\n",
      "/media/mountHDD2/thao/git/.env/lib/python3.10/site-packages/scipy/stats/_stats_py.py:1092: RuntimeWarning: overflow encountered in square\n",
      "  s = s**2\n",
      "/media/mountHDD2/thao/git/.env/lib/python3.10/site-packages/scipy/stats/_stats_py.py:1094: RuntimeWarning: overflow encountered in multiply\n",
      "  s *= a_zero_mean\n",
      "/media/mountHDD2/thao/git/.env/lib/python3.10/site-packages/numpy/core/_methods.py:187: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n",
      "/media/mountHDD2/thao/git/.env/lib/python3.10/site-packages/numpy/core/_methods.py:176: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n",
      "/media/mountHDD2/thao/git/.env/lib/python3.10/site-packages/scipy/stats/_stats_py.py:1088: RuntimeWarning: overflow encountered in square\n",
      "  s = a_zero_mean**2\n",
      "/media/mountHDD2/thao/git/.env/lib/python3.10/site-packages/numpy/core/_methods.py:118: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n"
     ]
    }
   ],
   "source": [
    "all_fea = []\n",
    "all_label = []\n",
    "\n",
    "for i in range (len(single_mat_paths)):\n",
    "    data_path = single_mat_paths[i]\n",
    "    a = h5py.File(data_path, 'r')\n",
    "    data_h5 = a['ecg']\n",
    "    data = np.array(data_h5)\n",
    "    clip_data = data[:, 500:5000]\n",
    "\n",
    "    list_features = []\n",
    "    \n",
    "    for i in range (12):\n",
    "        # list_features = []\n",
    "        data = clip_data[i]\n",
    "        \n",
    "        list_features.append(np.mean(data))\n",
    "        list_features.append(np.median(data))\n",
    "        list_features.append(np.std(data))\n",
    "        \n",
    "        list_features.append(np.max(data)-np.min(data))\n",
    "        \n",
    "        q3, q1 = np.percentile(data, [75 ,25])\n",
    "        list_features.append(q3 - q1)\n",
    "        \n",
    "        sk = scipy.stats.skew(data) \n",
    "        list_features.append(sk)\n",
    "        \n",
    "        kur = scipy.stats.kurtosis(data)\n",
    "        list_features.append(kur)\n",
    "    all_fea.append(list_features)\n",
    "\n",
    "    # data_fea = torch.tensor(data_fea)\n",
    "    # data_all = torch.cat((data_fea[0], data_fea[1], data_fea[2], data_fea[3], data_fea[3], data_fea[5], \n",
    "    #                       data_fea[6], data_fea[7], data_fea[8], data_fea[9], data_fea[10], data_fea[11]))\n",
    "\n",
    "    filename = data_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    label = main_df[main_df[\"File name\"] == filename][\"New Label\"].values.item()\n",
    "    all_label.append(label)\n",
    "\n",
    "# XG.fit(sig_train, label_train, eval_set=[(sig_test, label_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bc5ef31-05a2-4dfa-ae64-314db36597d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20835\n",
      "20835\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "print(len(all_label))\n",
    "print(len(all_fea))\n",
    "print(len(all_fea[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e1ae526-5b7e-4d0d-ad27-14f25f67922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "XG = XGBClassifier(device = device, learning_rate = lr)\n",
    "\n",
    "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "splits1 = sss.split(main_df[\"File name\"], main_df[\"New Label\"])\n",
    "for tr_idxs, vl_idxs in splits1:\n",
    "    pass\n",
    "tr_names, tr_label = main_df[\"File name\"][tr_idxs].tolist(),  main_df[\"New Label\"][tr_idxs].tolist()\n",
    "vl_names, vl_label = main_df[\"File name\"][vl_idxs].tolist(),  main_df[\"New Label\"][vl_idxs].tolist()\n",
    "\n",
    "# sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "# splits2 = sss.split(vl_names, vl_label)\n",
    "# for tr_idxs, vl_idxs in splits2:\n",
    "#     pass\n",
    "# vl_names, vl_label = vl_names[tr_idxs].tolist(),  vl_label[\"New Label\"][tr_idxs].tolist()\n",
    "# test_names, test_label = vl_names[vl_idxs].tolist(),  vl_label[\"New Label\"][vl_idxs].tolist()\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(all_label, all_fea, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "69c7b272-539d-40dd-9d0c-5fe18bd37622",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m q3, q1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpercentile(data, [\u001b[38;5;241m75\u001b[39m ,\u001b[38;5;241m25\u001b[39m])\n\u001b[1;32m     26\u001b[0m list_features\u001b[38;5;241m.\u001b[39mappend(q3 \u001b[38;5;241m-\u001b[39m q1)\n\u001b[0;32m---> 28\u001b[0m sk \u001b[38;5;241m=\u001b[39m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskew\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     29\u001b[0m list_features\u001b[38;5;241m.\u001b[39mappend(sk)\n\u001b[1;32m     31\u001b[0m kur \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mkurtosis(data)\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/scipy/stats/_axis_nan_policy.py:477\u001b[0m, in \u001b[0;36m_axis_nan_policy_factory.<locals>.axis_nan_policy_decorator.<locals>.axis_nan_policy_wrapper\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    476\u001b[0m     samples \u001b[38;5;241m=\u001b[39m _broadcast_arrays(samples, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m--> 477\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matleast_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m     n_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(axis)\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;66;03m# move all axes in `axis` to the end to be raveled\u001b[39;00m\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/numpy/core/shape_base.py:67\u001b[0m, in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     65\u001b[0m ary \u001b[38;5;241m=\u001b[39m asanyarray(ary)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ary\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 67\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     result \u001b[38;5;241m=\u001b[39m ary\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_fea = []\n",
    "train_label = []\n",
    "\n",
    "for i in range (len(single_mat_paths)):\n",
    "    data_path = single_mat_paths[i]\n",
    "    filename_train = data_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    if filename_train in tr_names:\n",
    "        a = h5py.File(data_path, 'r')\n",
    "        data_h5 = a['ecg']\n",
    "        data = np.array(data_h5, dtype=np.float64)\n",
    "        clip_data = data[:, 500:5000]\n",
    "    \n",
    "        list_features = []\n",
    "        \n",
    "        for i in range (12):\n",
    "            # list_features = []\n",
    "            data = clip_data[i]\n",
    "            \n",
    "            list_features.append(np.mean(data))\n",
    "            list_features.append(np.median(data))\n",
    "            list_features.append(np.std(data))\n",
    "            \n",
    "            list_features.append(np.max(data)-np.min(data))\n",
    "            \n",
    "            q3, q1 = np.percentile(data, [75 ,25])\n",
    "            list_features.append(q3 - q1)\n",
    "            \n",
    "            sk = scipy.stats.skew(data) \n",
    "            list_features.append(sk)\n",
    "            \n",
    "            kur = scipy.stats.kurtosis(data)\n",
    "            list_features.append(kur)\n",
    "        train_fea.append(list_features)\n",
    "    \n",
    "        # data_fea = torch.tensor(data_fea)\n",
    "        # data_all = torch.cat((data_fea[0], data_fea[1], data_fea[2], data_fea[3], data_fea[3], data_fea[5], \n",
    "        #                       data_fea[6], data_fea[7], data_fea[8], data_fea[9], data_fea[10], data_fea[11]))\n",
    "    \n",
    "        filename = data_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        label = main_df[main_df[\"File name\"] == filename][\"New Label\"].values.item()\n",
    "        train_label.append(label)\n",
    "    else: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48e8cd86-ca14-468c-af4f-45e199e3fd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fea = []\n",
    "test_label = []\n",
    "\n",
    "for i in range (len(single_mat_paths)):\n",
    "    data_path = single_mat_paths[i]\n",
    "    filename_train = data_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    if filename_train in vl_names:\n",
    "        a = h5py.File(data_path, 'r')\n",
    "        data_h5 = a['ecg']\n",
    "        data = np.array(data_h5)\n",
    "        clip_data = data[:, 500:5000]\n",
    "    \n",
    "        list_features = []\n",
    "        \n",
    "        for i in range (12):\n",
    "            # list_features = []\n",
    "            data = clip_data[i]\n",
    "            \n",
    "            list_features.append(np.mean(data))\n",
    "            list_features.append(np.median(data))\n",
    "            list_features.append(np.std(data))\n",
    "            \n",
    "            list_features.append(np.max(data)-np.min(data))\n",
    "            \n",
    "            q3, q1 = np.percentile(data, [75 ,25])\n",
    "            list_features.append(q3 - q1)\n",
    "            \n",
    "            sk = scipy.stats.skew(data) \n",
    "            list_features.append(sk)\n",
    "            \n",
    "            kur = scipy.stats.kurtosis(data)\n",
    "            list_features.append(kur)\n",
    "        test_fea.append(list_features)\n",
    "    \n",
    "        # data_fea = torch.tensor(data_fea)\n",
    "        # data_all = torch.cat((data_fea[0], data_fea[1], data_fea[2], data_fea[3], data_fea[3], data_fea[5], \n",
    "        #                       data_fea[6], data_fea[7], data_fea[8], data_fea[9], data_fea[10], data_fea[11]))\n",
    "    \n",
    "        filename = data_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        label = main_df[main_df[\"File name\"] == filename][\"New Label\"].values.item()\n",
    "        test_label.append(label)\n",
    "    else: \n",
    "        pass\n",
    "\n",
    "# X_val, X_test, y_val, y_test = train_test_split(test_fea, test_label, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9ee55412-dd88-40b3-82e2-4dbe75c895cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mountHDD2/thao/git/.env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/media/mountHDD2/thao/git/.env/lib/python3.10/site-packages/sklearn/utils/extmath.py:1070: RuntimeWarning: overflow encountered in square\n",
      "  temp **= 2\n",
      "/media/mountHDD2/thao/git/.env/lib/python3.10/site-packages/sklearn/utils/extmath.py:1076: RuntimeWarning: overflow encountered in square\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/mountHDD2/thao/git/.env/lib/python3.10/site-packages/sklearn/utils/extmath.py:1076: RuntimeWarning: invalid value encountered in subtract\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/mountHDD2/thao/git/.env/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:83: RuntimeWarning: overflow encountered in square\n",
      "  upper_bound = n_samples * eps * var + (n_samples * mean * eps) ** 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "train_fea = np.nan_to_num(train_fea)\n",
    "X_val = np.nan_to_num(X_val)\n",
    "# scaler = StandardScaler()\n",
    "# train_fea = scaler.fit_transform(train_fea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a2fe8190-7867-4b0f-9169-41d885dbb910",
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[16:26:45] /workspace/src/data/../common/../data/gradient_index.h:94: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\nStack trace:\n  [bt] (0) /media/mountHDD2/thao/git/.env/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x22dbbc) [0x76751062dbbc]\n  [bt] (1) /media/mountHDD2/thao/git/.env/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x52bcc9) [0x76751092bcc9]\n  [bt] (2) /media/mountHDD2/thao/git/.env/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x51b431) [0x76751091b431]\n  [bt] (3) /media/mountHDD2/thao/git/.env/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x51d82c) [0x76751091d82c]\n  [bt] (4) /media/mountHDD2/thao/git/.env/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x4cce3a) [0x7675108cce3a]\n  [bt] (5) /media/mountHDD2/thao/git/.env/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(XGQuantileDMatrixCreateFromCallback+0x18c) [0x76751054546c]\n  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x7676ef7a5e2e]\n  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x7676ef7a2493]\n  [bt] (8) /usr/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0xa3e9) [0x7676ef7c63e9]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mXG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_fea\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_label\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/xgboost/sklearn.py:1512\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_class\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_\n\u001b[1;32m   1511\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[0;32m-> 1512\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_evaluation_matrices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1515\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1518\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_qid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_dmatrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[1;32m   1532\u001b[0m     params,\n\u001b[1;32m   1533\u001b[0m     train_dmatrix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1542\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[1;32m   1543\u001b[0m )\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/xgboost/sklearn.py:596\u001b[0m, in \u001b[0;36m_wrap_evaluation_matrices\u001b[0;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_evaluation_matrices\u001b[39m(\n\u001b[1;32m    577\u001b[0m     missing: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    578\u001b[0m     X: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[1;32m    593\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, List[Tuple[Any, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;124;03m    way.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 596\u001b[0m     train_dmatrix \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dmatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m     n_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_set \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(eval_set)\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_or_none\u001b[39m(meta: Optional[Sequence], name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence:\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/xgboost/sklearn.py:1003\u001b[0m, in \u001b[0;36mXGBModel._create_dmatrix\u001b[0;34m(self, ref, **kwargs)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _can_use_qdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_method) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbooster \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1003\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQuantileDMatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnthread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_bin\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/xgboost/core.py:1573\u001b[0m, in \u001b[0;36mQuantileDMatrix.__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, data_split_mode)\u001b[0m\n\u001b[1;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   1554\u001b[0m         info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1555\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m         )\n\u001b[1;32m   1567\u001b[0m     ):\n\u001b[1;32m   1568\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1569\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf data iterator is used as input, data like label should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1570\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified as batch argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1571\u001b[0m         )\n\u001b[0;32m-> 1573\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/xgboost/core.py:1634\u001b[0m, in \u001b[0;36mQuantileDMatrix._init\u001b[0;34m(self, data, ref, enable_categorical, **meta)\u001b[0m\n\u001b[1;32m   1632\u001b[0m it\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;66;03m# delay check_call to throw intermediate exception first\u001b[39;00m\n\u001b[0;32m-> 1634\u001b[0m \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;241m=\u001b[39m handle\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/xgboost/core.py:284\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [16:26:45] /workspace/src/data/../common/../data/gradient_index.h:94: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\nStack trace:\n  [bt] (0) /media/mountHDD2/thao/git/.env/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x22dbbc) [0x76751062dbbc]\n  [bt] (1) /media/mountHDD2/thao/git/.env/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x52bcc9) [0x76751092bcc9]\n  [bt] (2) /media/mountHDD2/thao/git/.env/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x51b431) [0x76751091b431]\n  [bt] (3) /media/mountHDD2/thao/git/.env/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x51d82c) [0x76751091d82c]\n  [bt] (4) /media/mountHDD2/thao/git/.env/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x4cce3a) [0x7675108cce3a]\n  [bt] (5) /media/mountHDD2/thao/git/.env/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(XGQuantileDMatrixCreateFromCallback+0x18c) [0x76751054546c]\n  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x7676ef7a5e2e]\n  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x7676ef7a2493]\n  [bt] (8) /usr/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0xa3e9) [0x7676ef7c63e9]\n\n"
     ]
    }
   ],
   "source": [
    "XG.fit(train_fea, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3a68f5e9-768b-4ce0-90fd-f377e16ec1bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "need to call fit or load_model beforehand",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# XG.predict(X_test)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mXG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_fea\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_label\u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/sklearn/base.py:705\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;124;03mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;124;03m    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy_score(y, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/xgboost/sklearn.py:1565\u001b[0m, in \u001b[0;36mXGBClassifier.predict\u001b[0;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     X: ArrayLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1562\u001b[0m     iteration_range: Optional[IterationRange] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1563\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:\n\u001b[1;32m   1564\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbosity):\n\u001b[0;32m-> 1565\u001b[0m         class_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output_margin:\n\u001b[1;32m   1573\u001b[0m             \u001b[38;5;66;03m# If output_margin is active, simply return the scores\u001b[39;00m\n\u001b[1;32m   1574\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m class_probs\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/xgboost/sklearn.py:1186\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[0;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_use_inplace_predict():\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1186\u001b[0m         predts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39minplace_predict(\n\u001b[1;32m   1187\u001b[0m             data\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   1188\u001b[0m             iteration_range\u001b[38;5;241m=\u001b[39miteration_range,\n\u001b[1;32m   1189\u001b[0m             predict_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmargin\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_margin \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1190\u001b[0m             missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1191\u001b[0m             base_margin\u001b[38;5;241m=\u001b[39mbase_margin,\n\u001b[1;32m   1192\u001b[0m             validate_features\u001b[38;5;241m=\u001b[39mvalidate_features,\n\u001b[1;32m   1193\u001b[0m         )\n\u001b[1;32m   1194\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_alike(predts):\n\u001b[1;32m   1195\u001b[0m             \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=import-error\u001b[39;00m\n",
      "File \u001b[0;32m~/git/.env/lib/python3.10/site-packages/xgboost/sklearn.py:805\u001b[0m, in \u001b[0;36mXGBModel.get_booster\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__sklearn_is_fitted__():\n\u001b[1;32m    803\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NotFittedError\n\u001b[0;32m--> 805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneed to call fit or load_model beforehand\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\n",
      "\u001b[0;31mNotFittedError\u001b[0m: need to call fit or load_model beforehand"
     ]
    }
   ],
   "source": [
    "# XG.predict(X_test)\n",
    "XG.score(test_fea, test_label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f06ab0-6259-47d8-a3fb-dcc68dba0cef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
